import os
import csv
import time
import json
import requests
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from statistics import mean
import multiprocessing

# ==============================
# é…ç½®åŒº
# ==============================
OUTPUT_DIR = "output"
MIDDLE_DIR = os.path.join(OUTPUT_DIR, "middle")
LOG_DIR = os.path.join(OUTPUT_DIR, "log")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(MIDDLE_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

CSV_FILE = os.path.join(OUTPUT_DIR, "merge_total.csv")  # è¾“å…¥ CSV æ–‡ä»¶ï¼ˆ4åˆ—ï¼šé¢‘é“åã€åœ°å€ã€æ¥æºã€å›¾æ ‡ï¼‰
OUTPUT_M3U = os.path.join(OUTPUT_DIR, "working.m3u")
WORKING_CSV = os.path.join(OUTPUT_DIR, "working.csv")
PROGRESS_FILE = os.path.join(MIDDLE_DIR, "progress.json")
SKIPPED_FILE = os.path.join(LOG_DIR, "skipped.log")
SUSPECT_FILE = os.path.join(LOG_DIR, "suspect.log")

TIMEOUT = 15
BASE_THREADS = 50
MAX_THREADS = 200
BATCH_SIZE = 200
DEBUG = True

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0 Safari/537.36",
}

LOW_RES_KEYWORDS = ["vga", "480p", "576p"]
BLOCK_KEYWORDS = ["espanol"]
WHITELIST_PATTERNS = [".ctv", ".sdserver", ".sdn.", ".sda.", ".sdstream", "sdhd", "hdsd"]

# ==============================
# å·¥å…·å‡½æ•°
# ==============================
def log_skip(reason, title, url):
    with open(SKIPPED_FILE, "a", encoding="utf-8") as f:
        f.write(f"{reason} -> {title}\n{url}\n")

def log_suspect(reason, url):
    with open(SUSPECT_FILE, "a", encoding="utf-8") as f:
        f.write(f"{reason} -> {url}\n")

def is_allowed(title, url):
    text = f"{title} {url}".lower()
    if any(w in text for w in WHITELIST_PATTERNS):
        return True
    if any(kw in text for kw in LOW_RES_KEYWORDS):
        log_skip("LOW_RES", title, url)
        return False
    if any(kw in text for kw in BLOCK_KEYWORDS):
        log_skip("BLOCK_KEYWORD", title, url)
        return False
    return True

def quick_check(url):
    start = time.time()
    try:
        r = requests.head(url, headers=HEADERS, timeout=TIMEOUT, allow_redirects=True)
        elapsed = round(time.time() - start, 3)
        ctype = r.headers.get("content-type", "").lower()
        ok = r.status_code < 400 and any(v in ctype for v in [
            "video/", "mpegurl", "x-mpegurl",
            "application/vnd.apple.mpegurl",
            "application/x-mpegurl",
            "application/octet-stream"
        ])
        return ok, elapsed, r.url
    except Exception:
        return False, round(time.time() - start, 3), url

def ffprobe_check(url):
    start = time.time()
    try:
        cmd = [
            "ffprobe", "-v", "error",
            "-select_streams", "v:0",
            "-show_entries", "stream=codec_name",
            "-of", "json", url
        ]
        proc = subprocess.run(cmd, capture_output=True, text=True, timeout=TIMEOUT)
        data = json.loads(proc.stdout or "{}")
        ok = "streams" in data and len(data["streams"]) > 0
    except Exception:
        ok = False
    elapsed = round(time.time() - start, 3)
    return ok, elapsed, url

def test_stream(entry):
    title, url, source, logo = entry
    url = url.strip()
    try:
        ok, elapsed, final_url = quick_check(url)
        if not ok:
            ok, elapsed, final_url = ffprobe_check(url)
        return (ok, elapsed, final_url, title, source, logo)
    except Exception as e:
        log_skip("EXCEPTION", title, url)
        if DEBUG:
            print(f"âŒ EXCEPTION {title} -> {url} | {e}")
        return (False, 0, url, title, source, logo)

def detect_optimal_threads():
    test_urls = ["https://www.apple.com","https://www.google.com","https://www.microsoft.com"]
    times = []
    for u in test_urls:
        t0 = time.time()
        try:
            requests.head(u, timeout=TIMEOUT)
        except:
            pass
        times.append(time.time()-t0)
    avg = mean(times)
    cpu_threads = multiprocessing.cpu_count()*5
    if avg<0.5:
        return min(MAX_THREADS, cpu_threads)
    elif avg<1:
        return min(150, cpu_threads)
    elif avg<2:
        return min(100, cpu_threads)
    else:
        return BASE_THREADS

def write_working_csv(all_working):
    with open(WORKING_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        # è¡¨å¤´ï¼šé¢‘é“åã€åœ°å€ã€æ¥æºã€æ£€æµ‹æ—¶é—´ã€å›¾æ ‡
        writer.writerow(["é¢‘é“å", "åœ°å€", "æ¥æº", "æ£€æµ‹æ—¶é—´", "å›¾æ ‡"])
        for ok, elapsed, url, title, source, logo in all_working:
            if ok:
                writer.writerow([title, url, source, elapsed, logo])
    print(f"ðŸ“ ç”Ÿæˆ working.csv: {WORKING_CSV}")

# ==============================
# ä¸»é€»è¾‘
# ==============================
if __name__ == "__main__":
    # æ¸…ç©ºæ—¥å¿—
    for log_file in [SKIPPED_FILE, SUSPECT_FILE]:
        if os.path.exists(log_file):
            os.remove(log_file)

    # è¯»å– CSV å¹¶ç¡®è®¤åˆ—å
    pairs = []
    with open(CSV_FILE, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        print("CSV å­—æ®µ:", fieldnames)
        required_cols = ["é¢‘é“å", "åœ°å€", "æ¥æº", "å›¾æ ‡"]
        for col in required_cols:
            if col not in fieldnames:
                raise ValueError(f"CSV æ–‡ä»¶ç¼ºå°‘ required åˆ—: '{col}'")

        for row in reader:
            title = row.get("é¢‘é“å", "").strip()
            url = row.get("åœ°å€", "").strip()
            source = row.get("æ¥æº", "").strip()
            logo = row.get("å›¾æ ‡", "").strip()
            if title and url:
                pairs.append((title, url, source, logo))

    # è¿‡æ»¤
    filtered_pairs = [p for p in pairs if is_allowed(p[0], p[1])]
    print(f"ðŸš« è·³è¿‡æº: {len(pairs)-len(filtered_pairs)} æ¡")

    total = len(filtered_pairs)
    threads = detect_optimal_threads()
    print(f"âš™ï¸ åŠ¨æ€çº¿ç¨‹æ•°ï¼š{threads}")
    print(f"ðŸš€ å¼€å§‹æ£€æµ‹ {total} æ¡æµï¼Œæ¯æ‰¹ {BATCH_SIZE} æ¡")

    all_working = []
    start_time = time.time()
    done_index = 0

    if os.path.exists(PROGRESS_FILE):
        try:
            done_index = json.load(open(PROGRESS_FILE,encoding="utf-8")).get("done",0)
            print(f"ðŸ”„ æ¢å¤è¿›åº¦ï¼Œä»Žç¬¬ {done_index} æ¡ç»§ç»­")
        except:
            pass

    for batch_start in range(done_index, total, BATCH_SIZE):
        batch = filtered_pairs[batch_start:batch_start+BATCH_SIZE]
        with ThreadPoolExecutor(max_workers=threads) as executor:
            futures = {executor.submit(test_stream, entry): entry for entry in batch}
            for future in as_completed(futures):
                entry = futures[future]
                try:
                    ok, elapsed, final_url, title, source, logo = future.result()
                    if ok:
                        all_working.append((ok, elapsed, final_url, title, source, logo))
                        if DEBUG:
                            print(f"âœ… {title} ({elapsed}s)")
                    else:
                        log_skip("FAILED_CHECK", title, entry[1])
                except Exception as e:
                    log_skip("EXCEPTION", entry[0], entry[1])
        json.dump({"done": min(batch_start + BATCH_SIZE, total)}, open(PROGRESS_FILE, "w", encoding="utf-8"))
        print(f"ðŸ§® æœ¬æ‰¹å®Œæˆï¼š{len(all_working)}/{min(batch_start + BATCH_SIZE, total)} å¯ç”¨æµ | å·²å®Œæˆ {min(batch_start + BATCH_SIZE, total)}/{total}")

    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)

    if all_working:
        # å†™ M3U
        grouped = defaultdict(list)
        for ok, elapsed, url, title, source, logo in all_working:
            grouped[title.lower()].append((title, url, elapsed, source, logo))

        if os.path.exists(OUTPUT_M3U):
            os.remove(OUTPUT_M3U)

        with open(OUTPUT_M3U, "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            for name in sorted(grouped.keys()):
                group_sorted = sorted(grouped[name], key=lambda x: x[2])
                for title, url, _, _, _ in group_sorted:
                    f.write(f"#EXTINF:-1,{title}\n{url}\n")
        print(f"ðŸ“ å†™å…¥å®Œæˆ: {OUTPUT_M3U}")

        # å†™ working.csv
        write_working_csv(all_working)

    else:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨æµï¼Œworking.m3u å’Œ working.csv æœªæ›´æ–°")

    elapsed_total = round(time.time() - start_time, 2)
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œå…± {len(all_working)} æ¡å¯ç”¨æµï¼Œç”¨æ—¶ {elapsed_total} ç§’")
    print(f"âš ï¸ å¤±è´¥æˆ–è¿‡æ»¤æºæ—¥å¿—: {SKIPPED_FILE}")
    print(f"ðŸ•µï¸ å¯ç–‘è¯¯æ€æºæ—¥å¿—: {SUSPECT_FILE}")