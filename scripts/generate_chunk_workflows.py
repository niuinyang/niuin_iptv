import os
import glob
import re
from datetime import datetime, timedelta

WORKFLOW_DIR = ".github/workflows"
CHUNK_DIR = "output/chunk"

os.makedirs(WORKFLOW_DIR, exist_ok=True)

# è®¡ç®—æ¯å¤©ä»3:00èµ·ï¼Œæ¯ä¸ªchunkå»¶è¿Ÿçš„åˆ†é’Ÿæ•°ï¼Œæ­¥é•¿10åˆ†é’Ÿ
def get_cron_for_index(index):
    # indexä»0å¼€å§‹
    base_hour = 3  # 3ç‚¹
    base_minute = 0
    # æ¯ä¸ª chunk é—´éš”10åˆ†é’Ÿ
    total_minutes = base_minute + index * 10
    hour = base_hour + total_minutes // 60
    minute = total_minutes % 60
    # è¿”å›cronå­—ç¬¦ä¸²ï¼ŒUTCæ—¶é—´(ä¸œå…«åŒº3ç‚¹=UTC19ç‚¹)
    # æ³¨æ„GitHub Actions cronæ˜¯UTCæ—¶é—´
    # 3ç‚¹(åŒ—äº¬æ—¶é—´) = 19ç‚¹(UTC)
    utc_hour = (hour - 8) % 24  # è½¬æˆUTCå°æ—¶ï¼Œä¿è¯24å°æ—¶å†…å¾ªç¯
    return f"{minute} {utc_hour} * * *"

template = """name: Deep Validation Chunk {n}

on:
  schedule:
    - cron: '{cron}'  # æ¯å¤© UTC {utc_hour}:{minute:02d} è§¦å‘ï¼Œä¸œå…«åŒº{hour}: {minute:02d}ç‚¹
  workflow_dispatch:

permissions:
  contents: write

jobs:
  deep_chunk_{n}:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # ğŸ”§ å…è®¸å®Œæ•´æ‹‰å–å†å²è®°å½•ï¼Œç¡®ä¿èƒ½ rebase

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install system dependencies
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pillow tqdm chardet

      - name: Pull shared cache
        run: |
          mkdir -p output/cache
          git fetch origin ${{{{ github.ref }}}}
          git checkout origin/${{{{ github.ref }}}} -- output/cache/cache_hashes.json || echo "No shared cache found"

      - name: Run final scan on chunk {n}
        run: |
          python scripts/4.3final_scan.py \
            --input {chunk_file} \
            --output_dir output/chunk_final_scan \
            --cache_file output/cache/cache_hashes.json \
            --chunk_cache output/cache/chunk_{n}_hashes.json

      - name: Commit and push scan results safely
        env:
          REPO_TOKEN: ${{{{ secrets.PERSONAL_ACCESS_TOKEN }}}}
          GITHUB_REPOSITORY: ${{{{ github.repository }}}}
          GITHUB_REF: ${{{{ github.ref }}}}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # å¹¶å‘å®‰å…¨æ¨é€ï¼Œå¸¦é‡è¯•æœºåˆ¶
          for i in {{1..5}}; do
            git fetch origin ${{{{ github.ref }}}}
            git rebase origin/${{{{ github.ref }}}} || (git rebase --abort && sleep 10 && continue)
            git add output/chunk_final_scan/ output/cache/chunk_{n}_hashes.json
            git commit -m "ci: add final scan results chunk {n}" || true
            git pull --rebase origin ${{{{ github.ref }}}} || true
            git push https://x-access-token:${{{{REPO_TOKEN}}}}@github.com/${{{{GITHUB_REPOSITORY}}}} HEAD:${{{{GITHUB_REF}}}} && break
            echo "Push failed, retrying ($i)..."
            sleep $((RANDOM % 20 + 10))
          done

      - name: Self delete workflow file
        env:
          REPO_TOKEN: ${{{{ secrets.PERSONAL_ACCESS_TOKEN }}}}
          WORKFLOW_FILE: ".github/workflows/deep_chunk_{n}.yml"
          GITHUB_REPOSITORY: ${{{{ github.repository }}}}
          GITHUB_REF: ${{{{ github.ref }}}}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git fetch origin ${{{{ github.ref }}}}
          git rebase origin/${{{{ github.ref }}}} || git rebase --abort
          git rm "$WORKFLOW_FILE"
          git commit -m "ci: self delete workflow deep_chunk_{n}.yml after run"
          git pull --rebase origin ${{{{ github.ref }}}} || true
          git push https://x-access-token:${{{{REPO_TOKEN}}}}@github.com/${{{{GITHUB_REPOSITORY}}}} HEAD:${{{{GITHUB_REF}}}}
"""

chunk_files = sorted(glob.glob(os.path.join(CHUNK_DIR, "chunk_*.csv")))

for idx, chunk_file in enumerate(chunk_files):
    basename = os.path.basename(chunk_file)
    match = re.match(r"chunk_(\\d+)\\.csv", basename)
    if not match:
        print(f"è·³è¿‡ä¸åŒ¹é…çš„æ–‡ä»¶: {basename}")
        continue
    n = match.group(1)

    # è®¡ç®— cron è¡¨è¾¾å¼
    cron = get_cron_for_index(idx)
    # è®¡ç®—å¯¹åº”çš„æœ¬åœ°ä¸œå…«åŒºå°æ—¶å’Œåˆ†é’Ÿï¼Œç”¨äºæ³¨é‡Šï¼ˆä¾¿äºç†è§£ï¼‰
    total_minutes = idx * 10
    hour_local = 3 + total_minutes // 60
    minute_local = total_minutes % 60
    # è½¬æˆUTCå°æ—¶ï¼ˆ0-23ï¼‰
    utc_hour = (hour_local - 8) % 24

    wf_path = os.path.join(WORKFLOW_DIR, f"deep_chunk_{n}.yml")
    content = template.format(
        n=n,
        chunk_file=chunk_file,
        cron=cron,
        hour=hour_local,
        minute=minute_local,
        utc_hour=utc_hour,
        timestamp=datetime.now().astimezone().isoformat()
    )
    with open(wf_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"âœ… ç”Ÿæˆ workflow: {wf_path}")
