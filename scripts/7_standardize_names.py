#!/usr/bin/env python3
# standardize_iptv.py (CSV ç‰ˆæœ¬ï¼Œæ—  Excel)

import os
import re
import csv
import sys
import time
import chardet
import pandas as pd
from opencc import OpenCC
from rapidfuzz import fuzz, process
from tqdm import tqdm

# é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå…¨éƒ¨é‡‡ç”¨ CSV æ ¼å¼
MY_SUM_PATH = "output/middle/merge/mysource_total.csv"
WORKING_PATH = "output/middle/working.csv"
CHANNEL_DATA_PATH = "input/channel_data.csv"        # channel_data CSV æ–‡ä»¶è·¯å¾„ï¼ˆè¯»å†™ï¼‰
NETWORK_CHANNELS_PATH = "input/iptv-org/database/data/channels.csv"

OUTPUT_TOTAL_FINAL = "output/total_final.csv"       # æœ€ç»ˆè¾“å‡ºæ€»è¡¨è·¯å¾„
OUTPUT_CHANNEL_DATA = CHANNEL_DATA_PATH              # channel_data ä¿å­˜è·¯å¾„

cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“è½¬æ¢å™¨

# è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶è¯»å– CSV æ–‡ä»¶
def read_csv_auto_encoding(filepath):
    with open(filepath, 'rb') as f:
        raw = f.read(10000)
        result = chardet.detect(raw)
        encoding = result['encoding'] or 'utf-8'
    return pd.read_csv(filepath, encoding=encoding)

# æœºæ¢°å¼æ ‡å‡†åŒ–é¢‘é“åï¼ˆå»æ‹¬å·ã€ç©ºæ ¼ã€å°å†™ç­‰ï¼‰
def mechanical_standardize(name: str) -> str:
    if not isinstance(name, str):
        return ""
    s = name.strip()
    s = cc.convert(s)
    s = s.lower()
    s = re.sub(r"\ï¼ˆ.*?\ï¼‰", "", s)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\[.*?\]", "", s)
    s = re.sub(r"\ã€.*?\ã€‘", "", s)
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^a-z0-9\u4e00-\u9fa5\+\ï¼]", "", s)
    return s

# æ¸…ç†ç½‘ç»œé¢‘é“åº“ä¸­çš„é¢‘é“åæ ¼å¼ï¼ˆé¦–å­—æ¯å¤§å†™ç­‰ï¼‰
def clean_network_std_name(name: str) -> str:
    if not isinstance(name, str):
        return ""
    name = name.strip()
    name = re.sub(r"\s+", " ", name)
    name = ' '.join([
        w.capitalize() if re.match(r'[a-zA-Z]+$', w) else w
        for w in name.split(" ")
    ])
    return name

def main():
    print("å¼€å§‹è¯»å–æ–‡ä»¶...")

    # è¯»å–è¾“å…¥æ•°æ® my_sum å’Œ working
    my_sum = read_csv_auto_encoding(MY_SUM_PATH)
    working = read_csv_auto_encoding(WORKING_PATH)

    # å¦‚æœ channel_data æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºè¡¨æ ¼ï¼ˆå¸¦åŸºç¡€åˆ—ï¼‰
    if not os.path.exists(CHANNEL_DATA_PATH):
        pd.DataFrame(columns=["åŸå§‹å", "æ ‡å‡†å", "æ‹ŸåŒ¹é…é¢‘é“å", "åˆ†ç»„"]).to_csv(
            CHANNEL_DATA_PATH, index=False, encoding="utf-8-sig"
        )

    # è¯»å– channel_dataï¼Œå‡†å¤‡åç»­ä½¿ç”¨
    channel_data = read_csv_auto_encoding(CHANNEL_DATA_PATH)

    # æ–°å¢å¹¶å¡«å…… channel_data çš„ â€œæ¥æºâ€ åˆ—ï¼Œä»è¾“å…¥æ–‡ä»¶çš„æ¥æºå­—æ®µåŒ¹é…å¡«å……
    source_dict = {}
    for df in [my_sum, working]:
        for idx, row in df.iterrows():
            orig_name = row.get("é¢‘é“å", "")
            src = row.get("æ¥æº", "")
            if orig_name and src:
                if orig_name not in source_dict:
                    source_dict[orig_name] = src

    if "æ¥æº" not in channel_data.columns:
        channel_data["æ¥æº"] = channel_data["åŸå§‹å"].map(source_dict).fillna("")
    else:
        channel_data["æ¥æº"] = channel_data.apply(
            lambda row: source_dict.get(row["åŸå§‹å"], row["æ¥æº"]) if not row["æ¥æº"] else row["æ¥æº"],
            axis=1
        )

    # æ–°å¢â€œè¾“å‡ºé¡ºåºâ€åˆ—ï¼Œé»˜è®¤æ‰€æœ‰å€¼ä¸ºâ€œæœªæ’åºâ€
    if "è¾“å‡ºé¡ºåº" not in channel_data.columns:
        channel_data["è¾“å‡ºé¡ºåº"] = "æœªæ’åº"

    # æ–°å¢â€œæ˜¯å¦å·²ç»´æŠ¤â€åˆ—ï¼Œé»˜è®¤æ‰€æœ‰å€¼ä¸ºâ€œå¦â€
    if "æ˜¯å¦å·²ç»´æŠ¤" not in channel_data.columns:
        channel_data["æ˜¯å¦å·²ç»´æŠ¤"] = "å¦"

    # è¯»å–ç½‘ç»œé¢‘é“æ•°æ®åº“ï¼Œç¡®å®šé¢‘é“åæ‰€åœ¨åˆ—å¹¶å»ç©º
    network_channels_df = read_csv_auto_encoding(NETWORK_CHANNELS_PATH)
    if "channel" in network_channels_df.columns:
        network_col = "channel"
    elif "name" in network_channels_df.columns:
        network_col = "name"
    else:
        print("ç½‘ç»œæ•°æ®åº“æ— é¢‘é“ååˆ—ï¼")
        sys.exit(1)

    network_channels_df = network_channels_df.dropna(subset=[network_col])
    network_channels_df["std_key"] = network_channels_df[network_col].apply(mechanical_standardize)
    network_channels = dict(zip(network_channels_df["std_key"], network_channels_df[network_col]))

    # ç»Ÿä¸€è¾“å…¥æ•°æ®å­—æ®µï¼Œç¡®ä¿ä»¥ä¸‹å­—æ®µå­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ–°å»ºç©ºåˆ—
    for df in [my_sum, working]:
        for col in ["è§†é¢‘ç¼–ç ", "åˆ†è¾¨ç‡", "å¸§ç‡", "éŸ³é¢‘", "ç›¸ä¼¼åº¦"]:
            if col not in df.columns:
                df[col] = ""

    # åˆå¹¶ä¸¤ä¸ªè¾“å…¥æ•°æ®é›†ï¼Œåˆå¹¶åæ‰€æœ‰å­—æ®µé½å…¨
    total_before = pd.concat([my_sum, working], ignore_index=True, sort=False)

    # ç¡®ä¿æ‰€æœ‰å¿…é¡»å­—æ®µå­˜åœ¨ï¼Œä¸å­˜åœ¨è¡¥ç©º
    required_cols = ["é¢‘é“å", "åœ°å€", "æ¥æº", "å›¾æ ‡", "æ£€æµ‹æ—¶é—´",
                     "åˆ†ç»„", "è§†é¢‘ç¼–ç ", "åˆ†è¾¨ç‡", "å¸§ç‡", "éŸ³é¢‘", "ç›¸ä¼¼åº¦"]
    for col in required_cols:
        if col not in total_before.columns:
            total_before[col] = ""

    # æ–°å¢â€œè½®å›ç›¸ä¼¼åº¦â€åˆ—ï¼Œæ¥æºè¾“å…¥æ–‡ä»¶ï¼Œç¼ºå¤±å¡«â€œæ— â€
    if "è½®å›ç›¸ä¼¼åº¦" not in total_before.columns:
        total_before["è½®å›ç›¸ä¼¼åº¦"] = "æ— "
    else:
        total_before["è½®å›ç›¸ä¼¼åº¦"] = total_before["è½®å›ç›¸ä¼¼åº¦"].fillna("æ— ")

    # é¢‘é“åæ ‡å‡†åŒ–å­—æ®µï¼ˆç”¨äºåŒ¹é…ï¼‰
    total_before["std_key"] = total_before["é¢‘é“å"].apply(mechanical_standardize)

    # channel_data ä¸­æ·»åŠ æ ‡å‡†åŒ–è¾…åŠ©åˆ—ï¼Œæ–¹ä¾¿åŒ¹é…
    channel_data["æ ‡å‡†å_std_key"] = channel_data["æ ‡å‡†å"].apply(mechanical_standardize)
    channel_data["åŸå§‹å_std_key"] = channel_data["åŸå§‹å"].apply(mechanical_standardize)

    # ç°æœ‰åŸå§‹åé›†åˆï¼Œé¿å…é‡å¤æ–°å¢
    existing_orig_names = set(channel_data["åŸå§‹å"].fillna("").unique())

    # channel_data æ˜ å°„å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾æ ‡å‡†åå’Œæ‹ŸåŒ¹é…é¢‘é“å
    std_name_dict = dict(zip(channel_data["æ ‡å‡†å_std_key"], channel_data["æ ‡å‡†å"]))
    std_key_to_pending = dict(zip(channel_data["æ ‡å‡†å_std_key"], channel_data["æ‹ŸåŒ¹é…é¢‘é“å"]))

    # å‡†å¤‡æ˜ å°„æ ‡å‡†ååˆ°è¾“å‡ºé¡ºåºï¼Œæ–¹ä¾¿åç»­å¡«å……
    std_name_to_output_order = dict(zip(channel_data["æ ‡å‡†å"], channel_data["è¾“å‡ºé¡ºåº"]))

    # ç”¨äºå­˜å‚¨åŒ¹é…ç»“æœçš„åˆ—è¡¨
    matched_standard_names = []
    matched_match_info = []
    matched_match_score = []

    precise_match_count = 0
    fuzzy_match_count = 0

    # è¾…åŠ©å‡½æ•°ï¼šæ–°å¢ channel_data è®°å½•ï¼ˆé¿å…é‡å¤ï¼‰
    def add_channel_data_if_not_exists(orig_name, std_name, group_label):
        nonlocal channel_data, existing_orig_names
        if orig_name not in existing_orig_names:
            new_row = {
                "åŸå§‹å": orig_name,
                "æ ‡å‡†å": std_name,
                "æ‹ŸåŒ¹é…é¢‘é“å": std_name,
                "åˆ†ç»„": group_label,
                "æ¥æº": source_dict.get(orig_name, ""),
                "è¾“å‡ºé¡ºåº": "æœªæ’åº",
                "æ˜¯å¦å·²ç»´æŠ¤": "å¦"
            }
            channel_data = pd.concat(
                [channel_data, pd.DataFrame([new_row])],
                ignore_index=True
            )
            existing_orig_names.add(orig_name)

    print("å¼€å§‹åŒ¹é…æ ‡å‡†åŒ–é¢‘é“å...")

    total_len = len(total_before)
    batch_size = 50
    last_print_time = time.time()

    # æ‰¹é‡éå†ï¼Œé€æ¡åŒ¹é…é¢‘é“å
    for start_idx in tqdm(range(0, total_len, batch_size), desc="åŒ¹é…è¿›åº¦"):
        end_idx = min(start_idx + batch_size, total_len)
        batch = total_before.iloc[start_idx:end_idx]

        for idx, row in batch.iterrows():
            original_name = row["é¢‘é“å"]
            key = row["std_key"]

            matched_name = None
            match_info = "æœªåŒ¹é…"
            match_score = 0.0

            # ç²¾å‡†åŒ¹é…é€»è¾‘ï¼šå¿…é¡» channel_data ä¸­å­˜åœ¨åŸå§‹åä¸”â€œæ˜¯å¦å·²ç»´æŠ¤â€ä¸ºâ€œæ˜¯â€
            if original_name in existing_orig_names:
                matched_std_name = channel_data.loc[
                    channel_data["åŸå§‹å"] == original_name, "æ ‡å‡†å"
                ].values
                maintained_val = channel_data.loc[
                    channel_data["åŸå§‹å"] == original_name, "æ˜¯å¦å·²ç»´æŠ¤"
                ].values

                if len(matched_std_name) > 0 and len(maintained_val) > 0:
                    mv = maintained_val[0]
                    if isinstance(mv, str) and mv.strip() == "æ˜¯":
                        matched_name = matched_std_name[0]
                        match_info = "ç²¾å‡†åŒ¹é…"
                        match_score = 100.0
                        precise_match_count += 1
                    else:
                        matched_name = None
                else:
                    matched_name = None

            # æ¨¡ç³ŠåŒ¹é…é€»è¾‘
            if matched_name is None:
                choices = list(network_channels.keys())
                matches = process.extract(key, choices, scorer=fuzz.ratio, limit=1)

                if matches:
                    best_key, score, _ = matches[0]
                    if score > 90:
                        matched_name = clean_network_std_name(network_channels[best_key])
                        match_info = "æ¨¡ç³ŠåŒ¹é…ï¼ˆ>90%ï¼‰"
                        match_score = float(score)
                        fuzzy_match_count += 1
                        add_channel_data_if_not_exists(original_name, matched_name, "å¾…ç¡®è®¤åˆ†ç»„")
                    else:
                        matched_name = original_name
                        add_channel_data_if_not_exists(original_name, matched_name, "å¾…æ ‡å‡†åŒ–")
                else:
                    matched_name = original_name
                    add_channel_data_if_not_exists(original_name, matched_name, "å¾…æ ‡å‡†åŒ–")

            matched_standard_names.append(matched_name)
            matched_match_info.append(match_info)
            matched_match_score.append(match_score)

        if time.time() - last_print_time >= 5:
            print(f"å·²å¤„ç† {end_idx}/{total_len} æ¡ï¼Œç²¾å‡† {precise_match_count}ï¼Œæ¨¡ç³Š {fuzzy_match_count}")
            last_print_time = time.time()

    # æ›´æ–° total_before è¡¨ä¸­åŒ¹é…ç›¸å…³å­—æ®µ
    total_before["é¢‘é“å"] = matched_standard_names
    total_before["åŒ¹é…ä¿¡æ¯"] = matched_match_info
    total_before["åŒ¹é…å€¼"] = matched_match_score

    # ä» channel_data æ˜ å°„â€œåˆ†ç»„â€å­—æ®µ
    std_name_to_group = dict(zip(channel_data["æ ‡å‡†å"], channel_data["åˆ†ç»„"]))
    total_before["åˆ†ç»„"] = total_before["é¢‘é“å"].apply(lambda x: std_name_to_group.get(x, "æœªåˆ†ç±»"))

    # ä» channel_data æ˜ å°„â€œè¾“å‡ºé¡ºåºâ€å­—æ®µ
    total_before["è¾“å‡ºé¡ºåº"] = total_before["é¢‘é“å"].apply(lambda x: std_name_to_output_order.get(x, "æœªæ’åº"))

    # channel_data å»é‡ï¼Œä¿ç•™åŸå§‹åå”¯ä¸€çš„ç¬¬ä¸€æ¡
    channel_data = channel_data.drop_duplicates(subset=["åŸå§‹å"], keep='first')

    print("ä¿å­˜è¾“å‡ºæ–‡ä»¶...")

    # ä¿å­˜æ€»è¡¨ total_final.csvï¼ŒåŒ…å«æ–°å¢çš„è½®å›ç›¸ä¼¼åº¦å’Œè¾“å‡ºé¡ºåºåˆ—
    total_before.to_csv(
        OUTPUT_TOTAL_FINAL, index=False, encoding="utf-8-sig",
        columns=[
            "é¢‘é“å", "åœ°å€", "æ¥æº", "å›¾æ ‡", "æ£€æµ‹æ—¶é—´", "åˆ†ç»„",
            "è§†é¢‘ç¼–ç ", "åˆ†è¾¨ç‡", "å¸§ç‡", "éŸ³é¢‘", "ç›¸ä¼¼åº¦", "åŒ¹é…ä¿¡æ¯", "åŒ¹é…å€¼",
            "è½®å›ç›¸ä¼¼åº¦", "è¾“å‡ºé¡ºåº"
        ]
    )

    # ä¿å­˜ channel_data.csvï¼Œå¸¦æ–°å¢çš„ä¸‰åˆ—
    channel_data.to_csv(
        OUTPUT_CHANNEL_DATA, index=False, encoding="utf-8-sig",
        columns=["åŸå§‹å", "æ ‡å‡†å", "æ‹ŸåŒ¹é…é¢‘é“å", "åˆ†ç»„", "æ¥æº", "è¾“å‡ºé¡ºåº", "æ˜¯å¦å·²ç»´æŠ¤"]
    )

    print("ğŸ‰ CSV æ ‡å‡†åŒ–å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
