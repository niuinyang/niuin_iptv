#!/usr/bin/env python3
import csv
import os
import sys

# === è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿è·¯å¾„æ°¸è¿œæ­£ç¡® ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

def split_deep_scan(
        input_path=os.path.join(BASE_DIR, "output/middle/merge/networksource_total.csv"),
        chunk_size=1000,
        output_dir=os.path.join(BASE_DIR, "output/middle/chunk")
    ):
    """
    è¯»å– CSVï¼Œå°†å…¶æŒ‰æŒ‡å®šå¤§å°åˆ†å‰²æˆå¤šä¸ªåˆ†ç‰‡æ–‡ä»¶ chunk-N.csvã€‚
    è‡ªåŠ¨æ¸…ç†æ—§åˆ†ç‰‡æ–‡ä»¶ï¼Œè·¯å¾„åŸºäºè„šæœ¬å®é™…ä½ç½®ï¼Œé¿å… GitHub Actions è·¯å¾„é”™ä¹±ã€‚
    """

    print("=== è·¯å¾„æ£€æŸ¥ ===")
    print("è„šæœ¬æ‰€åœ¨ç›®å½• BASE_DIR:", BASE_DIR)
    print("å½“å‰å·¥ä½œç›®å½• os.getcwd():", os.getcwd())
    print("è¾“å…¥æ–‡ä»¶ç»å¯¹è·¯å¾„:", os.path.abspath(input_path))
    print("chunk è¾“å‡ºç›®å½•ç»å¯¹è·¯å¾„:", os.path.abspath(output_dir))

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_path):
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ - {input_path}")
        sys.exit(1)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # === åˆ é™¤ output/middle/chunk ä¸­æ—§çš„åˆ†ç‰‡æ–‡ä»¶ ===
    print("\n=== æ¸…ç†æ—§çš„åˆ†ç‰‡æ–‡ä»¶ ===")
    for filename in os.listdir(output_dir):
        full_path = os.path.join(output_dir, filename)
        print(f"å‘ç°æ–‡ä»¶: {full_path}")

        # åˆ é™¤ chunk-*.csv
        if filename.startswith("chunk") and filename.endswith(".csv"):
            os.remove(full_path)
            print(f"ğŸ‘‰ å·²åˆ é™¤: {full_path}")
        else:
            print(f"âŒ è·³è¿‡ï¼ˆä¸æ˜¯ chunk*.csvï¼‰: {full_path}")

    # === è¯»å– CSV æ–‡ä»¶ ===
    print("\n=== è¯»å– CSV æ–‡ä»¶ ===")
    try:
        with open(input_path, newline='', encoding="utf-8") as f:
            reader = csv.DictReader(f)
            headers = reader.fieldnames
            rows = list(reader)
    except UnicodeDecodeError:
        print("UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç ...")
        import chardet
        with open(input_path, "rb") as f:
            data = f.read()
            detected = chardet.detect(data)
            encoding = detected.get("encoding", "utf-8")

        print(f"æ£€æµ‹åˆ°ç¼–ç : {encoding}")

        text = data.decode(encoding, errors="ignore")
        rows = list(csv.DictReader(text.splitlines()))
        headers = rows[0].keys() if rows else []

    total_rows = len(rows)
    print(f"è¯»å–è¡Œæ•°: {total_rows}")

    # === å¼€å§‹æ‹†åˆ† ===
    total_chunks = (total_rows + chunk_size - 1) // chunk_size
    print(f"é¢„è®¡ç”Ÿæˆ {total_chunks} ä¸ªåˆ†ç‰‡æ–‡ä»¶")

    for start in range(0, total_rows, chunk_size):
        chunk_rows = rows[start:start + chunk_size]
        chunk_index = start // chunk_size + 1
        chunk_name = f"chunk-{chunk_index}.csv"
        chunk_path = os.path.join(output_dir, chunk_name)

        with open(chunk_path, "w", newline='', encoding="utf-8") as cf:
            writer = csv.DictWriter(cf, fieldnames=headers)
            writer.writeheader()
            writer.writerows(chunk_rows)

        print(f"âœ” å·²ç”Ÿæˆ: {chunk_path}ï¼ˆè¡Œæ•° {len(chunk_rows)}ï¼‰")

    print("\nğŸ‰ æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶å·²å®Œæˆ")


if __name__ == "__main__":
    split_deep_scan()
