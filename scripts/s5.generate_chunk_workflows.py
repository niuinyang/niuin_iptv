#!/usr/bin/env python3
# scripts/generate_chunk_workflows.py
"""
ä¸º output/middle/chunk/ ä¸‹çš„ chunk{main}-{sub}.csv è‡ªåŠ¨ç”Ÿæˆç‹¬ç«‹çš„ GitHub Actions workflow
æ¯ä¸ª workflow é—´éš” 5 åˆ†é’Ÿï¼ŒæŒ‰é¡ºåºè¿è¡Œ:
  1) scripts/4.1fast_scan.py  -> output/middle/fast/fast_chunk{main}-{sub}.csv
  2) scripts/4.2deep_scan.py  -> output/middle/deep/deep_chunk{main}-{sub}.csv
  3) scripts/4.3final_scan.py  -> output/middle/final/final_chunk{main}-{sub}.csv
æ¨é€é€»è¾‘æ²¿ç”¨åŸè„šæœ¬ï¼ˆå¸¦ GITHUB_TOKEN å®‰å…¨æ¨é€ + é‡è¯•/æ‹‰å–åˆå¹¶ï¼‰ã€‚
"""

import os
import re
import json
import time
import subprocess

WORKFLOW_DIR = ".github/workflows"
CHUNK_DIR = "output/middle/chunk"
CACHE_FILE = "output/cache_workflow.json"

# åˆ›å»ºéœ€è¦çš„ç›®å½•
os.makedirs(WORKFLOW_DIR, exist_ok=True)
os.makedirs("output/cache", exist_ok=True)
os.makedirs("output/middle/fast", exist_ok=True)
os.makedirs("output/middle/deep", exist_ok=True)
os.makedirs("output/middle/final", exist_ok=True)

# ğŸ§© æ¨¡æ¿ï¼šæ³¨æ„å¯¹ GITHUB_TOKEN çš„è½¬ä¹‰ï¼ˆåœ¨ format ä¸­ä¿ç•™ GitHub Actions çš„èŠ±æ‹¬å·ï¼‰
TEMPLATE = """name: Scan Chunk {file_base}

on:
  schedule:
    - cron: '{cron}'  # æ¯å¤© UTC {utc_hour}:{utc_min:02d} è§¦å‘
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scan_chunk_{job_name}:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install ffmpeg
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run fast scan
        run: |
          set -e
          mkdir -p output/middle/fast
          python scripts/4.1fast_scan.py --input {chunk_path} --output output/middle/fast/fast_{file_base}.csv

      - name: Run deep scan
        run: |
          set -e
          mkdir -p output/middle/deep
          python scripts/4.2deep_scan.py --input output/middle/fast/fast_{file_base}.csv --output output/middle/deep/deep_{file_base}.csv

      - name: Run final scan
        run: |
          set -e
          mkdir -p output/middle/final
          python scripts/4.3final_scan.py --input output/middle/deep/deep_{file_base}.csv --output output/middle/final/final_{file_base}.csv --cache_dir output/cache

      - name: Commit and push changes
        env:
          GITHUB_TOKEN: ${{{{ secrets.GITHUB_TOKEN }}}}
        run: |
          set -e || true
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # åªæ·»åŠ ç›¸å…³è¾“å‡ºæ–‡ä»¶å’Œç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          git add output/middle/final/final_{file_base}.csv output/cache || true
          git commit -m "ci: add scan results for {file_base}" || echo "No changes to commit"

          # è®¾ç½®è¿œç¨‹å¹¶å¸¦å®‰å…¨æ¨é€é‡è¯•æœºåˆ¶
          git remote set-url origin https://x-access-token:${{GITHUB_TOKEN}}@github.com/niuinyang/niuin_iptv.git

          for i in 1 2 3; do
            echo "æ¨é€å°è¯•ç¬¬ $i æ¬¡"
            if git push origin HEAD:main; then
              echo "æ¨é€æˆåŠŸ âœ…"
              break
            else
              echo "æ¨é€å¤±è´¥ï¼Œå°è¯•æ‹‰å–è¿œç¨‹åˆå¹¶ ğŸ”„"
              git stash push -m "ci: stash before pull" || echo "stash å¤±è´¥æˆ–æ— å˜æ›´"
              if git pull --rebase origin main; then
                echo "æ‹‰å–æˆåŠŸï¼Œå‡†å¤‡é‡è¯•æ¨é€"
                git stash pop || echo "æ—  stash å¯å¼¹å‡ºæˆ– pop å¤±è´¥"
              else
                echo "æ‹‰å–å¤±è´¥ï¼Œç­‰å¾… 30 ç§’åé‡è¯•"
                git rebase --abort || true
                git stash pop || echo "æ—  stash å¯å¼¹å‡º"
                sleep 30
              fi
            fi
          done
"""

# ğŸ§¹ æ¸…ç†æ—§çš„ scan_chunk_*.yml workflow æ–‡ä»¶
print("ğŸ§¹ æ¸…ç†æ—§çš„ workflow æ–‡ä»¶ï¼ˆscan_chunk_*.ymlï¼‰...")
for f in os.listdir(WORKFLOW_DIR):
    if re.match(r"scan_chunk_[\d\-]+\.yml", f):
        try:
            os.remove(os.path.join(WORKFLOW_DIR, f))
        except Exception as e:
            print(f"åˆ é™¤ {f} å¤±è´¥: {e}")

# å¦‚æœå­˜åœ¨æ—§ç¼“å­˜æ–‡ä»¶ï¼Œå…ˆåˆ é™¤ï¼ˆé‡æ–°ç”Ÿæˆï¼‰
if os.path.exists(CACHE_FILE):
    try:
        os.remove(CACHE_FILE)
    except Exception as e:
        print(f"åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

# ğŸ•’ å®šæ—¶åˆ†é…: èµ·å§‹æ—¶é—´ï¼ˆUTCï¼‰
start_hour = 19  # UTC åŸºå‡†å°æ—¶ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
start_minute = 30
interval = 5  # æ¯ä¸ª chunk ç›¸éš” 5 åˆ†é’Ÿ

# æ”¶é›† chunk æ–‡ä»¶ï¼ˆåªåŒ¹é… chunk{main}-{sub}.csvï¼‰
chunks = []
pattern = re.compile(r'^chunk(\d+)-(\d+)\.csv$', re.IGNORECASE)
if os.path.isdir(CHUNK_DIR):
    for f in os.listdir(CHUNK_DIR):
        m = pattern.match(f)
        if m:
            main = int(m.group(1))
            sub = int(m.group(2))
            chunks.append((main, sub, f))
else:
    print(f"âš ï¸ æ‰¾ä¸åˆ°ç›®å½• {CHUNK_DIR}ï¼Œè¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    chunks = []

# æŒ‰ä¸»ç¼–å·ã€å­ç¼–å·æ’åºï¼ˆä¿è¯é¡ºåºï¼‰
chunks.sort(key=lambda x: (x[0], x[1]))
total_chunks = len(chunks)
print(f"æ‰¾åˆ° {total_chunks} ä¸ª chunk æ–‡ä»¶ï¼Œå¼€å§‹ç”Ÿæˆ workflows...")

cache_data = {}

for idx, (main, sub, filename) in enumerate(chunks, start=1):
    # è®¡ç®— cron æ—¶é—´
    total_minutes = start_minute + (idx - 1) * interval
    utc_hour = start_hour + (total_minutes // 60)
    utc_min = total_minutes % 60
    # å¦‚æœè·¨å¤©ï¼Œæ¨¡ 24
    if utc_hour >= 24:
        utc_hour = utc_hour % 24
    cron = f"{utc_min} {utc_hour} * * *"

    # æ–‡ä»¶ä¸åå­—
    file_base = f"chunk{main}-{sub}"
    job_name = f"{main}_{sub}"
    workflow_filename = f"scan_chunk_{main}-{sub}.yml"
    workflow_path = os.path.join(WORKFLOW_DIR, workflow_filename)
    chunk_path = os.path.join(CHUNK_DIR, filename).replace("\\", "/")  # ä¿è¯è·¯å¾„æ ¼å¼åœ¨ windows ä¸‹ä¹Ÿ ok

    # å†™å…¥ workflow æ–‡ä»¶
    try:
        with open(workflow_path, "w", encoding="utf-8") as f:
            f.write(TEMPLATE.format(
                file_base=file_base,
                cron=cron,
                utc_hour=utc_hour,
                utc_min=utc_min,
                job_name=job_name,
                chunk_path=chunk_path
            ))
        print(f"âœ… å·²ç”Ÿæˆ workflow: {workflow_filename}  è§¦å‘æ—¶é—´: {cron}")
    except Exception as e:
        print(f"âœ–ï¸ å†™å…¥ {workflow_filename} å¤±è´¥: {e}")
        continue

    cache_data[file_base] = {"cron": cron, "workflow": workflow_filename, "source": chunk_path}

# å†™å…¥ç¼“å­˜æ–‡ä»¶
try:
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“¦ å·²å†™å…¥ç¼“å­˜æ–‡ä»¶: {CACHE_FILE}")
except Exception as e:
    print(f"å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

# ğŸŒ€ æäº¤ç”Ÿæˆçš„ workflow å’Œç¼“å­˜æ–‡ä»¶åˆ° GitHubï¼ˆæœ¬åœ°æ‰§è¡Œ git æ“ä½œï¼‰
print("\nğŸŒ€ å°è¯•æäº¤å¹¶æ¨é€ç”Ÿæˆçš„ workflow å’Œç¼“å­˜æ–‡ä»¶åˆ° GitHub...\n")

subprocess.run(["git", "add", "-A"], check=False)
subprocess.run(["git", "status"], check=False)
commit_msg = "ci: auto-generate scan chunk workflows"
result = subprocess.run(["git", "commit", "-m", commit_msg], text=True)
if result.returncode == 0:
    print("âœ… å·²æäº¤æ›´æ”¹ï¼Œå‡†å¤‡æ¨é€...")
else:
    print("â„¹ï¸ æ— æ›´æ”¹æˆ–æäº¤å¤±è´¥ï¼Œè·³è¿‡æäº¤")

# å¤šæ¬¡æ¨é€é‡è¯•ï¼ˆé˜²æ­¢å¶å‘å†²çªï¼‰
for attempt in range(1, 4):
    print(f"å°è¯•æ¨é€ï¼Œç¬¬ {attempt} æ¬¡...")
    code = subprocess.run(["git", "push"], text=True).returncode
    if code == 0:
        print("ğŸš€ æ¨é€æˆåŠŸ")
        break
    else:
        print("âš ï¸ æ¨é€å¤±è´¥ï¼Œç­‰å¾… 30 ç§’åé‡è¯•...")
        time.sleep(30)

print("\nğŸ¯ å®Œæˆã€‚")
