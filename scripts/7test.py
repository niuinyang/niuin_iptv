#!/usr/bin/env python3
import csv
import os
import pandas as pd
from rapidfuzz import process
import re
import chardet
from pypinyin import lazy_pinyin
from tqdm import tqdm
import time

IPTV_DB_PATH = "./iptv-database"

INPUT_MY = "input/mysource/my_sum.csv"
INPUT_WORKING = "output/working.csv"
OUTPUT_TOTAL = "output/total.csv"
INPUT_CHANNEL = "input/channel.csv"
OUTPUT_CHANNEL = "input/channel.csv"
MANUAL_MAP_PATH = "input/manual_map.csv"

def convert_file_to_utf8(path):
    if not os.path.exists(path):
        print(f"âš ï¸ æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œè·³è¿‡è½¬æ¢")
        return
    with open(path, 'rb') as f:
        raw = f.read()
    result = chardet.detect(raw)
    enc = result['encoding']
    if enc is None:
        enc = 'utf-8'
    if enc.lower() != 'utf-8-sig':
        try:
            text = raw.decode(enc, errors='ignore')
            with open(path, 'w', encoding='utf-8-sig') as f:
                f.write(text)
            print(f"âœ… æ–‡ä»¶ {path} ä» {enc} è½¬ç ä¸º UTF-8-SIG")
        except Exception as e:
            print(f"âŒ è½¬ç æ–‡ä»¶ {path} å‡ºé”™: {e}")
    else:
        print(f"âœ… æ–‡ä»¶ {path} å·²ç»æ˜¯ UTF-8-SIGï¼Œæ— éœ€è½¬æ¢")

def convert_all_csv_to_utf8(paths):
    for p in paths:
        convert_file_to_utf8(p)

def read_csv_auto_encoding(path, dtype=None):
    """
    å…ˆè½¬ä¸º utf-8-sig ç¼–ç æ–‡ä»¶ï¼Œå†ç”¨ utf-8-sig è¯»å–ã€‚
    dtype å¯é€‰ä¼ å…¥ã€‚
    """
    convert_file_to_utf8(path)
    return pd.read_csv(path, encoding='utf-8-sig', dtype=dtype)

def safe_read_csv(path):
    return read_csv_auto_encoding(path)

def load_name_map():
    name_map = {}
    path = os.path.join(IPTV_DB_PATH, "data", "channels.csv")
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            std_name = row["name"].strip().title()
            name_map[std_name.lower()] = std_name
            others = row.get("other_names", "")
            for alias in others.split(","):
                alias = alias.strip()
                if alias:
                    name_map[alias.lower()] = std_name
    return name_map

def load_manual_map(path=MANUAL_MAP_PATH):
    manual_map = {}
    if not os.path.exists(path):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8-sig", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"])
        print(f"âš ï¸ æœªæ‰¾åˆ°äººå·¥æ˜ å°„æ–‡ä»¶ï¼Œå·²æ–°å»ºç©ºæ–‡ä»¶ï¼š{path}")
        return manual_map

    df = read_csv_auto_encoding(path, dtype=str)
    for _, row in df.iterrows():
        raw_name = row.get("åŸå§‹åç§°")
        std_name = row.get("æ ‡å‡†åç§°")

        if pd.isna(raw_name) or pd.isna(std_name):
            continue

        raw_name_str = str(raw_name).strip()
        std_name_str = str(std_name).strip().title()

        if raw_name_str and std_name_str:
            manual_map[raw_name_str.lower()] = std_name_str
    return manual_map

def clean_channel_name(name):
    if not isinstance(name, str):
        return ""
    name = re.sub(r"[\(\ï¼ˆ][^\)\ï¼‰]*[\)\ï¼‰]", "", name)
    name = re.sub(r"[\[\ã€][^\]\ã€‘]*[\]\ã€‘]", "", name)
    name = re.sub(r"\b(not\s*)?(24/7|7\*24|7x24)\b", "", name, flags=re.I)
    return name.strip()

def normalize_name_for_match(name):
    if not isinstance(name, str):
        return ""
    name = clean_channel_name(name)
    name = re.sub(r"[-\s]", "", name)
    return name.lower()

def standardize_my_sum(my_sum_df):
    my_sum_df['final_name'] = my_sum_df.iloc[:,0].astype(str).str.title()
    my_sum_df['match_info'] = "è‡ªæœ‰æº"
    my_sum_df['original_channel_name'] = my_sum_df.iloc[:,0].astype(str)
    return my_sum_df

def standardize_working(working_df, my_sum_df, name_map, manual_map):
    working_df['original_channel_name'] = working_df.iloc[:, 0].astype(str)
    working_df['clean_name'] = working_df['original_channel_name'].apply(clean_channel_name)
    my_name_dict = dict(zip(my_sum_df.iloc[:,0].apply(normalize_name_for_match), my_sum_df['final_name']))

    total = len(working_df)
    final_names = []
    match_infos = []
    matched_count = 0
    unmatched_count = 0

    print(f"ğŸ”„ å¼€å§‹å¯¹ working.csv å…± {total} æ¡è®°å½•è¿›è¡Œæ ‡å‡†åŒ–åŒ¹é…...")

    start_time = time.time()
    last_print_time = start_time

    for idx, (orig_name, clean_name) in enumerate(tqdm(zip(working_df['original_channel_name'], working_df['clean_name']), total=total), 1):
        orig_name_lower = orig_name.lower()
        clean_name_lower = normalize_name_for_match(clean_name)

        if orig_name_lower in manual_map:
            std_name = manual_map[orig_name_lower]
            match_info = "äººå·¥åŒ¹é…"
            matched_count += 1
        elif clean_name_lower in my_name_dict:
            std_name = my_name_dict[clean_name_lower]
            match_info = "è‡ªæœ‰æºåŒ¹é…"
            matched_count += 1
        else:
            choices = list(name_map.keys())
            match, score, _ = process.extractOne(clean_name_lower, choices)
            if score >= 95:
                std_name = name_map[match]
                match_info = "æ¨¡ç³ŠåŒ¹é…"
                matched_count += 1
            elif score > 0:
                std_name = clean_name.title()
                match_info = f"ä½åŒ¹é…;æ‹ŸåŒ¹é…é¢‘é“:{name_map[match]}"
                unmatched_count += 1
            else:
                std_name = clean_name.title()
                match_info = "æœªåŒ¹é…"
                unmatched_count += 1

        final_names.append(std_name)
        match_infos.append(match_info)

        current_time = time.time()
        if current_time - last_print_time >= 5 or idx == total:
            print(f"å·²å¤„ç† {idx}/{total} æ¡ï¼ŒåŒ¹é… {matched_count} æ¡ï¼ŒæœªåŒ¹é… {unmatched_count} æ¡")
            last_print_time = current_time

    working_df['final_name'] = final_names
    working_df['match_info'] = match_infos
    print("âœ… working.csv æ ‡å‡†åŒ–åŒ¹é…å®Œæˆ")
    return working_df

def export_unmatched_for_manual(working_df, manual_map_path=MANUAL_MAP_PATH):
    unmatched_mask = working_df['match_info'].fillna("").str.contains("æœªåŒ¹é…|ä½åŒ¹é…", na=False)
    unmatched_df = working_df[unmatched_mask].copy()

    def extract_candidate(info):
        if not isinstance(info, str):
            return ""
        m = re.search(r"æ‹ŸåŒ¹é…é¢‘é“:([^\s,ï¼Œ]+)", info)
        if m:
            return m.group(1).strip()
        return ""

    export_df = pd.DataFrame({
        "åŸå§‹åç§°": unmatched_df['original_channel_name'].astype(str).str.strip(),
        "æ ‡å‡†åç§°": "",
        "æ‹ŸåŒ¹é…é¢‘é“": unmatched_df['match_info'].apply(extract_candidate).astype(str).str.strip()
    }).drop_duplicates(subset=["åŸå§‹åç§°"], keep="first")

    if export_df.empty:
        if not os.path.exists(manual_map_path):
            os.makedirs(os.path.dirname(manual_map_path), exist_ok=True)
            pd.DataFrame(columns=["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"]).to_csv(manual_map_path, index=False, encoding="utf-8-sig")
        print(f"ğŸ”” æ— æ–°å¢æœªåŒ¹é…æˆ–ä½åŒ¹é…é¢‘é“ï¼Œå·²ç¡®ä¿ {manual_map_path} å­˜åœ¨ã€‚")
        return

    if os.path.exists(manual_map_path):
        existing = pd.read_csv(manual_map_path, encoding="utf-8-sig", dtype=str)
    else:
        existing = pd.DataFrame(columns=["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"])

    for col in ["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"]:
        if col not in existing.columns:
            existing[col] = ""

    existing = existing[["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"]].astype(str)

    combined = pd.concat([existing, export_df], ignore_index=True)
    combined.drop_duplicates(subset=["åŸå§‹åç§°"], keep="first", inplace=True)

    os.makedirs(os.path.dirname(manual_map_path), exist_ok=True)
    combined.to_csv(manual_map_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ”” å·²æ›´æ–° {manual_map_path}ï¼Œå…± {len(combined)} æ¡è®°å½•ã€‚")

def sort_by_name_pinyin(df, col_name):
    df['_sort_key'] = df[col_name].apply(lambda x: ''.join(lazy_pinyin(str(x).lower())))
    df = df.sort_values(by='_sort_key').drop(columns=['_sort_key'])
    return df.reset_index(drop=True)

def sort_channel_file(path=OUTPUT_CHANNEL):
    if not os.path.exists(path):
        print(f"âš ï¸ æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œæ— æ³•æ’åº")
        return

    df = pd.read_csv(path, encoding="utf-8-sig")
    df['åˆ†ç»„'] = df['åˆ†ç»„'].fillna("").replace("", "æœªåˆ†ç±»")

    group_order = [
        "å¤®è§†é¢‘é“",
        "4Ké¢‘é“",
        "å«è§†é¢‘é“",
        "å±±ä¸œé¢‘é“",
        "ä»–çœé¢‘é“",
        "æ•°å­—é¢‘é“",
        "ç”µå°å¹¿æ’­",
        "å›½é™…é¢‘é“"
    ]

    def group_rank(g):
        if g == "æœªåˆ†ç±»":
            return 9999
        try:
            return group_order.index(g)
        except ValueError:
            return 9998

    df['åˆ†ç»„æ’åºæƒé‡'] = df['åˆ†ç»„'].apply(group_rank)
    df = df.sort_values(by=['åˆ†ç»„æ’åºæƒé‡']).reset_index(drop=True)

    result_frames = []
    for g, group_df in df.groupby('åˆ†ç»„', sort=False):
        sorted_group = sort_by_name_pinyin(group_df, 'é¢‘é“å')
        result_frames.append(sorted_group)

    df_sorted = pd.concat(result_frames, ignore_index=True)
    df_sorted = df_sorted.drop(columns=['åˆ†ç»„æ’åºæƒé‡'])

    df_sorted.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å¯¹ {path} è¿›è¡Œåˆ†ç»„åŠé¢‘é“åæ‹¼éŸ³æ’åº")

def sort_manual_map_file(path=MANUAL_MAP_PATH):
    if not os.path.exists(path):
        print(f"âš ï¸ æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œæ— æ³•æ’åº")
        return

    df = pd.read_csv(path, encoding="utf-8-sig")
    df['æ ‡å‡†åç§°æ˜¯å¦ç©º'] = df['æ ‡å‡†åç§°'].fillna("").apply(lambda x: 1 if x.strip() == "" else 0)
    df = df.sort_values(by=['æ ‡å‡†åç§°æ˜¯å¦ç©º']).reset_index(drop=True)

    df_has_std = df[df['æ ‡å‡†åç§°æ˜¯å¦ç©º'] == 0].copy()
    df_no_std = df[df['æ ‡å‡†åç§°æ˜¯å¦ç©º'] == 1].copy()

    df_has_std = sort_by_name_pinyin(df_has_std, 'åŸå§‹åç§°')
    df_no_std = sort_by_name_pinyin(df_no_std, 'åŸå§‹åç§°')

    df_sorted = pd.concat([df_has_std, df_no_std], ignore_index=True)
    df_sorted = df_sorted.drop(columns=['æ ‡å‡†åç§°æ˜¯å¦ç©º'])

    df_sorted.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å¯¹ {path} è¿›è¡Œæ ‡å‡†åç§°ä¼˜å…ˆåŠåŸå§‹åç§°æ‹¼éŸ³æ’åº")

def build_total_df(df):
    def safe_col(name_list):
        for name in name_list:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * len(df))

    # è¿™é‡Œæ–°å¢è§†é¢‘ç¼–ç ã€åˆ†è¾¨ç‡ã€å¸§ç‡ã€éŸ³é¢‘ã€ç›¸ä¼¼åº¦åˆ—
    return pd.DataFrame({
        "é¢‘é“å": df.get("final_name", df.iloc[:, 0]),
        "åœ°å€": safe_col(["åœ°å€"]),
        "æ¥æº": safe_col(["æ¥æº"]),
        "æ£€æµ‹æ—¶é—´": safe_col(["æ£€æµ‹æ—¶é—´", "æ£€æµ‹æ—¶é—´(å»¶è¿Ÿ)"]),
        "å›¾æ ‡": safe_col(["å›¾æ ‡"]),
        "åˆ†ç»„": safe_col(["åˆ†ç»„"]),
        "åŒ¹é…ä¿¡æ¯": safe_col(["match_info"]),
        "åŸå§‹é¢‘é“å": safe_col(["original_channel_name"]),
        "è§†é¢‘ç¼–ç ": safe_col(["è§†é¢‘ç¼–ç "]),
        "åˆ†è¾¨ç‡": safe_col(["åˆ†è¾¨ç‡"]),
        "å¸§ç‡": safe_col(["å¸§ç‡"]),
        "éŸ³é¢‘": safe_col(["éŸ³é¢‘"]),
        "ç›¸ä¼¼åº¦": safe_col(["ç›¸ä¼¼åº¦"]),
    })

def save_standardized_my_sum(df):
    def safe_col(name_list):
        for name in name_list:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * len(df))

    out_df = pd.DataFrame({
        "é¢‘é“å": df.get("final_name", df.iloc[:, 0]),
        "åœ°å€": safe_col(["åœ°å€"]),
        "æ¥æº": safe_col(["æ¥æº"]),
        "æ£€æµ‹æ—¶é—´": safe_col(["æ£€æµ‹æ—¶é—´", "æ£€æµ‹æ—¶é—´(å»¶è¿Ÿ)"]),
        "å›¾æ ‡": safe_col(["å›¾æ ‡"]),
        "åˆ†ç»„": safe_col(["åˆ†ç»„"]),
        "åŒ¹é…ä¿¡æ¯": safe_col(["match_info"]),
        "åŸå§‹é¢‘é“å": safe_col(["original_channel_name"]),
        "è§†é¢‘ç¼–ç ": safe_col(["è§†é¢‘ç¼–ç "]),
        "åˆ†è¾¨ç‡": safe_col(["åˆ†è¾¨ç‡"]),
        "å¸§ç‡": safe_col(["å¸§ç‡"]),
        "éŸ³é¢‘": safe_col(["éŸ³é¢‘"]),
        "ç›¸ä¼¼åº¦": safe_col(["ç›¸ä¼¼åº¦"]),
    })
    out_df.to_csv("input/mysource/my_sum_standardized.csv", index=False, encoding="utf-8-sig")
    print("âœ… å·²ä¿å­˜æ–‡ä»¶ï¼šinput/mysource/my_sum_standardized.csv")

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ ‡å‡†åŒ–åŒ¹é…æµç¨‹...")

    csv_files = [INPUT_MY, INPUT_WORKING, INPUT_CHANNEL, MANUAL_MAP_PATH]
    convert_all_csv_to_utf8(csv_files)

    my_sum_df = safe_read_csv(INPUT_MY)
    working_df = safe_read_csv(INPUT_WORKING)

    print(f"è¯»å–æºæ–‡ä»¶ï¼š\n  ğŸ“ {INPUT_MY}\n  ğŸ“ {INPUT_WORKING}")

    name_map = load_name_map()
    manual_map = load_manual_map()
    print(f"âœ… æ•°æ®åº“åŠ è½½å®Œæˆï¼Œæ˜ å°„æ€»æ•°ï¼š{len(name_map)}ï¼Œäººå·¥æ˜ å°„æ¡æ•°ï¼š{len(manual_map)}")

    my_sum_df = standardize_my_sum(my_sum_df)
    save_standardized_my_sum(my_sum_df)

    working_df = standardize_working(working_df, my_sum_df, name_map, manual_map)

    export_unmatched_for_manual(working_df)

    my_sum_out = build_total_df(my_sum_df)
    working_out = build_total_df(working_df)

    total_df = pd.concat([my_sum_out, working_out], ignore_index=True)

    # ===== æ–°å¢ï¼šæ ¹æ®é¢‘é“åä» channel.csv æ›´æ–° total_df çš„â€œåˆ†ç»„â€åˆ— =====
    if os.path.exists(INPUT_CHANNEL):
        channel_df = pd.read_csv(INPUT_CHANNEL, encoding="utf-8-sig")
        channel_group_map = dict(zip(channel_df["é¢‘é“å"].str.lower(), channel_df["åˆ†ç»„"].fillna("").astype(str)))

        def update_group(row):
            name = row["é¢‘é“å"]
            if not isinstance(name, str):
                return row["åˆ†ç»„"]
            lower_name = name.lower()
            if lower_name in channel_group_map and channel_group_map[lower_name].strip() != "":
                return channel_group_map[lower_name]
            else:
                return row["åˆ†ç»„"]

        total_df["åˆ†ç»„"] = total_df.apply(update_group, axis=1)
    # ===============================================================

    total_df.to_csv(OUTPUT_TOTAL, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜æ–‡ä»¶ï¼š{OUTPUT_TOTAL}ï¼Œå…±è®¡ {len(total_df)} æ¡è®°å½•")

    if os.path.exists(INPUT_CHANNEL):
        existing_channel_df = pd.read_csv(INPUT_CHANNEL, encoding="utf-8-sig")
    else:
        existing_channel_df = pd.DataFrame(columns=["é¢‘é“å", "åˆ†ç»„"])

    manual_map_lower = {k.lower(): v for k, v in manual_map.items()}
    def replace_name(row):
        old_name_lower = row["é¢‘é“å"].lower()
        if old_name_lower in manual_map_lower:
            return manual_map_lower[old_name_lower]
        return row["é¢‘é“å"]

    existing_channel_df["é¢‘é“å"] = existing_channel_df.apply(replace_name, axis=1)
    existing_channel_df.drop_duplicates(subset=["é¢‘é“å"], keep="first", inplace=True)

    total_channels = total_df[["é¢‘é“å", "åˆ†ç»„"]]
    existing_names = set(existing_channel_df["é¢‘é“å"])

    new_channels_df = total_channels[~total_channels["é¢‘é“å"].isin(existing_names)].copy()
    new_channels_df["åˆ†ç»„"] = "æœªåˆ†ç±»"

    combined_channel_df = pd.concat([existing_channel_df, new_channels_df], ignore_index=True)
    combined_channel_df.drop_duplicates(subset=["é¢‘é“å"], keep="first", inplace=True)

    combined_channel_df.to_csv(OUTPUT_CHANNEL, index=False, encoding="utf-8-sig")

    added = len(new_channels_df)
    modified = len(existing_channel_df)
    print(f"âœ… æ›´æ–° channel.csv å®Œæˆï¼Œæ–°å¢é¢‘é“æ•°ï¼š{added}ï¼Œç°æœ‰é¢‘é“æ•°ï¼ˆå»é‡åï¼‰ï¼š{modified}")

    sort_channel_file(OUTPUT_CHANNEL)
    sort_manual_map_file(MANUAL_MAP_PATH)
    print("âœ… manual_map.csv æ’åºå®Œæˆ")

if __name__ == "__main__":
    main()