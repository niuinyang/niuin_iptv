#!/usr/bin/env python3
import os
import re
import json

WORKFLOW_DIR = ".github/workflows"        # GitHub Actions å·¥ä½œæµæ–‡ä»¶å­˜æ”¾ç›®å½•
CHUNK_DIR = "output/middle/chunk"         # å­˜æ”¾åˆ†ç‰‡ CSV æ–‡ä»¶çš„ç›®å½•
CACHE_FILE = "output/cache_workflow.json" # ç”Ÿæˆçš„ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œè®°å½•æ‰€æœ‰ workflow æ–‡ä»¶ä¿¡æ¯

# ç¡®ä¿å·¥ä½œæµç›®å½•å’Œç¼“å­˜ç›®å½•å­˜åœ¨ï¼Œé¿å…å†™æ–‡ä»¶æ—¶å‡ºé”™
os.makedirs(WORKFLOW_DIR, exist_ok=True)
os.makedirs("output/cache", exist_ok=True)

# GitHub Actions workflow æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ {n} å ä½ç¬¦æ›¿æ¢åˆ†ç‰‡ç¼–å·
# åŒ…å«ä¸‰é˜¶æ®µæ‰«æè„šæœ¬ä¾æ¬¡æ‰§è¡Œçš„æ­¥éª¤ï¼Œæœ€åä¼šæäº¤å¹¶æ¨é€ç»“æœæ–‡ä»¶
# å…¶ä¸­ env é‡Œè®¾ç½® COMMIT_SHA å˜é‡ï¼Œæ–¹ä¾¿è¿½è¸ªä»£ç ç‰ˆæœ¬
TEMPLATE = """name: Scan_{n}

on:
  workflow_run:
    workflows: ["4ç”Ÿæˆchunk_workflows"]
    types:
      - completed
  workflow_dispatch:

env:
  COMMIT_SHA: ${{{{ github.sha }}}}

permissions:
  contents: write

jobs:
  scan_{n}:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install ffmpeg
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run fast scan for {n}
        run: |
          mkdir -p output/middle/fast/ok output/middle/fast/not
          python scripts/5.1fast_scan.py \\
            --input output/middle/chunk/{n}.csv \\
            --output output/middle/fast/ok/fast_{n}.csv \\
            --invalid output/middle/fast/not/fast_{n}-invalid.csv
            
      - name: Run deep scan for {n}
        run: |
          mkdir -p output/middle/deep/ok output/middle/deep/not
          python scripts/5.2deep_scan.py \\
            --input output/middle/fast/ok/fast_{n}.csv \\
            --output output/middle/deep/ok/deep_{n}.csv \\
            --invalid output/middle/deep/not/deep_{n}-invalid.csv

      - name: Run final scan for {n}
        run: |
          mkdir -p output/middle/final/ok output/middle/final/not
          python scripts/5.3final_scan.py \\
            --input output/middle/deep/ok/deep_{n}.csv \\
            --output output/middle/final/ok/final_{n}.csv \\
            --invalid output/middle/final/not/final_{n}-invalid.csv \\
            --cache_dir output/cache

      - name: Commit and Push Outputs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add output/cache \\
                  output/middle/fast \\
                  output/middle/deep \\
                  output/middle/final

          if git diff --cached --quiet; then
            echo "No output updates."
            exit 0
          fi

          git commit -m "Update scan outputs for {n} [skip ci]"

          MAX_RETRIES=5
          COUNT=1

          until git push --quiet; do
            echo "Push failed (attempt $COUNT/$MAX_RETRIES), retrying..."

            git stash push -m "auto-stash" || true
            git pull --rebase --quiet || true
            git stash pop || true

            COUNT=$((COUNT+1))
            if [ $COUNT -gt $MAX_RETRIES ]; then
              echo "ğŸ”¥ Push failed after $MAX_RETRIES attempts."
              exit 1
            fi

            sleep 2
          done

          echo "Push outputs succeeded."
"""

# æ‰“å°æ¸…ç†æç¤ºä¿¡æ¯
print("ğŸ§¹ æ¸…ç†æ—§çš„ workflow æ–‡ä»¶...")

# éå† workflow ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ é™¤ç¬¦åˆ scan_*.yml å‘½åè§„åˆ™çš„æ—§ workflow æ–‡ä»¶
for f in os.listdir(WORKFLOW_DIR):
    if re.match(r"scan_.+\.yml", f):
        os.remove(os.path.join(WORKFLOW_DIR, f))

# å¦‚æœå­˜åœ¨ç¼“å­˜æ–‡ä»¶ï¼Œåˆ é™¤å®ƒï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆ
if os.path.exists(CACHE_FILE):
    os.remove(CACHE_FILE)

# è·å– chunk ç›®å½•ä¸‹æ‰€æœ‰ç¬¦åˆ chunk-æ•°å­—.csv æ ¼å¼çš„æ–‡ä»¶ï¼Œæ’åºï¼Œæ–¹ä¾¿ä¾æ¬¡ç”Ÿæˆ workflow
chunks = sorted([f for f in os.listdir(CHUNK_DIR) if re.match(r"chunk-?\d+\.csv", f)])

cache_data = {}  # ç”¨äºç¼“å­˜æ‰€æœ‰ç”Ÿæˆçš„ workflow ä¿¡æ¯ï¼Œä¾¿äºåç»­ä½¿ç”¨å’Œç®¡ç†

for chunk_file in chunks:
    chunk_id = os.path.splitext(chunk_file)[0]  # å»é™¤æ‰©å±•åï¼Œåªä¿ç•™æ–‡ä»¶åéƒ¨åˆ†ï¼Œå¦‚ chunk-22

    workflow_filename = f"scan_{chunk_id}.yml"  # ç”Ÿæˆ workflow æ–‡ä»¶å
    workflow_path = os.path.join(WORKFLOW_DIR, workflow_filename)  # workflow æ–‡ä»¶å®Œæ•´è·¯å¾„

    # å°†æ¨¡æ¿ä¸­çš„å ä½ç¬¦ {n} æ›¿æ¢æˆå½“å‰ chunk_idï¼Œå†™å…¥å¯¹åº”çš„ workflow æ–‡ä»¶
    with open(workflow_path, "w", encoding="utf-8") as f:
        f.write(TEMPLATE.format(n=chunk_id))

    # æ‰“å°æç¤ºï¼Œå‘ŠçŸ¥å·²ç”Ÿæˆå¯¹åº”çš„ workflow æ–‡ä»¶
    print(f"âœ… å·²ç”Ÿæˆ workflow: {workflow_filename}")

    # æŠŠå½“å‰ workflow æ–‡ä»¶åå­˜å…¥ç¼“å­˜å­—å…¸ï¼Œåç»­å†™å…¥ç¼“å­˜æ–‡ä»¶
    cache_data[chunk_id] = {"file": workflow_filename}

# å°†æ‰€æœ‰ç”Ÿæˆçš„ workflow ä¿¡æ¯å†™å…¥ç¼“å­˜ JSON æ–‡ä»¶
with open(CACHE_FILE, "w", encoding="utf-8") as f:
    json.dump(cache_data, f, indent=2, ensure_ascii=False)

# æ‰“å°å®Œæˆæç¤ºï¼Œæé†’ç”¨æˆ·æäº¤å¹¶æ¨é€
print("\nğŸŒ€ ç”Ÿæˆ workflow å’Œç¼“å­˜æ–‡ä»¶å®Œæˆã€‚è¯·æäº¤å¹¶æ¨é€ã€‚")