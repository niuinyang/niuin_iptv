import aiohttp
import asyncio
import csv
import os
import subprocess
import json
import time
import tempfile
from PIL import Image
import imagehash

# ==============================
# é…ç½®åŒº
# ==============================
INPUT_FILE = "output/merge_total.csv"
OUTPUT_DIR = "output"
WORKING_FILE = os.path.join(OUTPUT_DIR, "working.csv")
WORKING_M3U = os.path.join(OUTPUT_DIR, "working.m3u")
LOG_DIR = os.path.join(OUTPUT_DIR, "log")
os.makedirs(LOG_DIR, exist_ok=True)

SKIPPED_FILE = os.path.join(LOG_DIR, "skipped.log")
ERROR_FILE = os.path.join(LOG_DIR, "error.log")
FAKE_FILE = os.path.join(LOG_DIR, "fake.log")

MAX_CONCURRENCY = 10  # ffprobeå’Œffmpegè¾ƒè€—èµ„æºï¼Œé€‚å½“è°ƒä½
TIMEOUT = 8           # è¶…æ—¶æ—¶é—´(ç§’)
FAKE_HASH_THRESHOLD = 5  # å¸§å“ˆå¸Œå·®å¼‚é˜ˆå€¼ï¼Œå°äºæ­¤åˆ™è§†ä¸ºå‡æº

LOW_RES_KEYWORDS = [
    "vga", "270p", "360p", "396p", "406p", "480p",
    "540p", "576p", "576i", "614p", "720p", "sd"
]
BLOCK_KEYWORDS = ["espanol"]
WHITELIST_PATTERNS = [".ctv", ".sdserver", ".sdn.", ".sda.", ".sdstream", "sdhd", "hdsd"]


# ==============================
# å·¥å…·å‡½æ•°
# ==============================
def log_to_file(path, msg):
    with open(path, "a", encoding="utf-8") as f:
        f.write(msg + "\n")


def is_allowed(title, url):
    text = f"{title} {url}".lower()
    if any(w in text for w in WHITELIST_PATTERNS):
        return True
    if any(kw in text for kw in LOW_RES_KEYWORDS):
        log_to_file(SKIPPED_FILE, f"LOW_RESOLUTION_FILTER -> {title} | {url}")
        return False
    if any(kw in text for kw in BLOCK_KEYWORDS):
        log_to_file(SKIPPED_FILE, f"BLOCK_KEYWORD -> {title} | {url}")
        return False
    return True


# ==============================
# ç¬¬1æ­¥ HTTP HEAD æ£€æµ‹
# ==============================
async def http_head_check(session, url):
    try:
        async with session.head(url, timeout=TIMEOUT) as resp:
            if resp.status == 200:
                return True
    except Exception as e:
        log_to_file(ERROR_FILE, f"HTTP HEADå¤±è´¥: {url} | {str(e)}")
    return False


# ==============================
# ç¬¬2æ­¥ FFprobeæ£€æµ‹
# ==============================
async def ffprobe_check(url):
    cmd = [
        "ffprobe",
        "-v", "error",
        "-print_format", "json",
        "-show_format",
        "-show_streams",
        url
    ]
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    stdout, stderr = await proc.communicate()
    if proc.returncode != 0:
        log_to_file(ERROR_FILE, f"FFprobeå¤±è´¥: {url} | {stderr.decode(errors='ignore')}")
        return None
    try:
        info = json.loads(stdout.decode())
    except Exception as e:
        log_to_file(ERROR_FILE, f"FFprobe JSONè§£æå¤±è´¥: {url} | {str(e)}")
        return None
    streams = info.get("streams", [])
    has_video = any(s.get("codec_type") == "video" for s in streams)
    return info if has_video else None


# ==============================
# ç¬¬3æ­¥ æŠ“å–æµæ•°æ®æ£€æµ‹
# ==============================
async def fetch_stream_data(session, url):
    try:
        async with session.get(url, timeout=TIMEOUT) as resp:
            if resp.status == 200:
                data = await resp.content.read(4096)
                if b"#EXTM3U" in data or b"TS" in data:
                    return True
    except Exception as e:
        log_to_file(ERROR_FILE, f"æŠ“å–æµæ•°æ®å¤±è´¥: {url} | {str(e)}")
    return False


# ==============================
# ç¬¬4æ­¥ æ’­æ”¾æ¨¡æ‹Ÿæ£€æµ‹
# ==============================
def simulate_playback(url):
    cmd = [
        "ffmpeg",
        "-y",
        "-i", url,
        "-t", "5",
        "-f", "null",
        "-"
    ]
    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if proc.returncode != 0:
        log_to_file(ERROR_FILE, f"æ’­æ”¾æ¨¡æ‹Ÿå¤±è´¥: {url} | {proc.stderr.decode(errors='ignore')}")
        return False
    return True


# ==============================
# ç¬¬5æ­¥ å‡æºæ£€æµ‹ï¼ˆå¸§å“ˆå¸Œåˆ†æï¼‰
# ==============================
def is_fake_stream(url, threshold=FAKE_HASH_THRESHOLD):
    tmpdir = tempfile.mkdtemp()
    cmd = [
        "ffmpeg", "-y", "-i", url,
        "-vf", "select='eq(pict_type,I)'",
        "-frames:v", "5",
        os.path.join(tmpdir, "frame_%02d.jpg")
    ]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    hashes = []
    for f in sorted(os.listdir(tmpdir)):
        path = os.path.join(tmpdir, f)
        try:
            h = imagehash.phash(Image.open(path))
            hashes.append(h)
        except Exception:
            continue

    diffs = [abs(hashes[i] - hashes[i+1]) for i in range(len(hashes)-1)]
    avg_diff = sum(diffs)/len(diffs) if diffs else 0

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for f in os.listdir(tmpdir):
        os.remove(os.path.join(tmpdir, f))
    os.rmdir(tmpdir)

    # å¹³å‡å·®å¼‚è¿‡ä½è¡¨ç¤ºç”»é¢å‡ ä¹ä¸å˜
    if avg_diff < threshold:
        return True
    return False


# ==============================
# ç»¼åˆæ£€æµ‹æµç¨‹
# ==============================
async def full_check(session, sem, row):
    async with sem:
        name, url, source, logo = row
        if not is_allowed(name, url):
            return None

        start_time = time.perf_counter()

        # 1. HTTPæ£€æµ‹
        if not await http_head_check(session, url):
            return None

        # 2. FFprobeæ£€æµ‹
        info = await ffprobe_check(url)
        if not info:
            return None

        # 3. æŠ“å–æµæ•°æ®æ£€æµ‹
        if not await fetch_stream_data(session, url):
            return None

        # 4. æ’­æ”¾æ¨¡æ‹Ÿæ£€æµ‹
        if not simulate_playback(url):
            return None

        # 5. å‡æºæ£€æµ‹
        if is_fake_stream(url):
            log_to_file(FAKE_FILE, f"å‡æº -> {name} | {url}")
            return None

        elapsed = time.perf_counter() - start_time
        detect_time = f"{elapsed:.2f}s"
        print(f"âœ… é€šè¿‡æ£€æµ‹: {name} | è€—æ—¶ {detect_time}")
        return [name, url, source, logo, detect_time, "ç½‘ç»œæº"]


# ==============================
# ä¸»ä»»åŠ¡
# ==============================
async def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        required_cols = ["é¢‘é“å", "åœ°å€", "æ¥æº", "å›¾æ ‡"]
        for col in required_cols:
            if col not in reader.fieldnames:
                raise ValueError(f"CSV æ–‡ä»¶ç¼ºå°‘åˆ—: '{col}'")
        rows = [[r["é¢‘é“å"], r["åœ°å€"], r["æ¥æº"], r["å›¾æ ‡"]] for r in reader]

    print(f"ğŸ“Š è¯»å–æº: {len(rows)} æ¡")

    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    async with aiohttp.ClientSession() as session:
        tasks = [full_check(session, sem, row) for row in rows]
        results = await asyncio.gather(*tasks)

    working = [r for r in results if r]
    print(f"\nâœ… æœ‰æ•ˆæº: {len(working)} æ¡")

    with open(WORKING_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["é¢‘é“å", "åœ°å€", "æ¥æº", "å›¾æ ‡", "æ£€æµ‹æ—¶é—´", "åˆ†ç»„"])
        writer.writerows(working)

    with open(WORKING_M3U, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for name, url, source, logo, detect_time, group in working:
            f.write(f'#EXTINF:-1 tvg-logo="{logo}",{name}\n{url}\n')

    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{WORKING_FILE} å’Œ {WORKING_M3U}")
    print(f"ğŸ•’ æ£€æµ‹å®Œæˆï¼Œå…± {len(working)} æ¡æœ‰æ•ˆæºã€‚")


if __name__ == "__main__":
    start = time.time()
    asyncio.run(main())
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {time.time() - start:.2f} ç§’")