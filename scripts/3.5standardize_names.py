#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import csv
import pandas as pd
import requests
from rapidfuzz import process

IPTV_DB_URL = "https://raw.githubusercontent.com/iptv-org/database/master/data/channels.csv"
IPTV_DB_FILE = "channels.csv"
OUTPUT_DIR = "output"
INPUT_CHANNEL_CSV = "input/channel.csv"
THRESHOLD = 95  # åŒ¹é…é˜ˆå€¼

match_cache = {}

def update_database():
    if os.path.exists(IPTV_DB_FILE):
        print(f"âœ… æ•°æ®åº“æ–‡ä»¶ {IPTV_DB_FILE} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return
    print("ğŸ”½ æ­£åœ¨ä¸‹è½½æœ€æ–° channels.csv ...")
    try:
        r = requests.get(IPTV_DB_URL, timeout=30)
        r.raise_for_status()
        with open(IPTV_DB_FILE, "wb") as f:
            f.write(r.content)
        print("âœ… æ•°æ®åº“ä¸‹è½½å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        if not os.path.exists(IPTV_DB_FILE):
            raise SystemExit("âŒ æ²¡æœ‰å¯ç”¨çš„é¢‘é“æ•°æ®åº“")

def load_name_map():
    """åŠ è½½ç½‘ç»œåº“é¢‘é“ååŠåˆ«åæ˜ å°„"""
    name_map = {}
    with open(IPTV_DB_FILE, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            std = row["name"].strip()
            name_map[std.lower()] = std
            aliases = row.get("aliases", "") or row.get("other_names", "")
            for alias in aliases.replace("|", ",").split(","):
                alias = alias.strip()
                if alias:
                    name_map[alias.lower()] = std
    print(f"ğŸ“š å·²åŠ è½½ {len(name_map)} ä¸ªåç§°æ˜ å°„")
    return name_map

def read_csv_auto(path, encodings=None):
    if encodings is None:
        encodings = ['utf-8-sig', 'utf-8', 'gbk', 'latin1']
    for enc in encodings:
        try:
            df = pd.read_csv(path, encoding=enc)
            print(f"âœ… ä½¿ç”¨ç¼–ç  {enc} è¯»å–æ–‡ä»¶æˆåŠŸ: {path}")
            return df
        except UnicodeDecodeError:
            print(f"âš ï¸ ç¼–ç  {enc} è¯»å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
    raise UnicodeDecodeError(f"æ— æ³•ç”¨å¤‡é€‰ç¼–ç è¯»å–æ–‡ä»¶: {path}")

def build_local_name_map(df_my_sum):
    """ä»my_sum.csvæ„å»ºæœ¬åœ°é¢‘é“åæ˜ å°„ï¼Œkeyä¸ºå°å†™ï¼Œvalueä¸ºåŸå"""
    local_map = {}
    for name in df_my_sum.iloc[:, 0].astype(str):
        local_map[name.lower()] = name
    return local_map

def match_name_working(name, local_map, network_map):
    """
    å¯¹working.csvé¢‘é“ååŒ¹é…ï¼š
    1. å…ˆå’Œæœ¬åœ°my_sumåæ¨¡ç³ŠåŒ¹é…
    2. å¦‚æœæœªè¾¾é˜ˆå€¼ï¼Œå†å’Œç½‘ç»œåº“æ¨¡ç³ŠåŒ¹é…
    """
    n = name.strip()
    if not n:
        return n, "ç©ºå"
    if n in match_cache:
        return match_cache[n]

    key = n.lower()

    # 1. æœ¬åœ°åº“æ¨¡ç³ŠåŒ¹é…
    local_candidates = list(local_map.keys())
    match_local, score_local, _ = process.extractOne(key, local_candidates)
    if score_local >= THRESHOLD:
        res = (local_map[match_local], f"æœ¬åœ°åº“æ¨¡ç³ŠåŒ¹é…({score_local:.0f})")
        match_cache[n] = res
        return res

    # 2. ç½‘ç»œåº“åŒ¹é…
    if key in network_map:
        res = (network_map[key], "ç½‘ç»œåº“ç²¾ç¡®åŒ¹é…")
        match_cache[n] = res
        return res

    network_candidates = list(network_map.keys())
    match_net, score_net, _ = process.extractOne(key, network_candidates)
    if score_net >= THRESHOLD:
        res = (network_map[match_net], f"ç½‘ç»œåº“æ¨¡ç³ŠåŒ¹é…({score_net:.0f})")
    else:
        res = (n, f"åŒ¹é…åº¦ä½({max(score_local, score_net):.0f})ï¼Œä¿ç•™åŸå")

    match_cache[n] = res
    return res

def standardize_my_sum(path):
    """my_sum.csvä¸åŒ¹é…ï¼Œç›´æ¥è¾“å‡ºåŸåä½œä¸ºæ ‡å‡†å"""
    print(f"ğŸ“‚ æ­£åœ¨è¯»å– my_sum.csv (ä¸åŒ¹é…): {path}")
    df = read_csv_auto(path)
    df.insert(0, "æ ‡å‡†é¢‘é“å", df.iloc[:, 0].astype(str))
    df.insert(1, "åŒ¹é…çŠ¶æ€", ["æœªåŒ¹é…-è·³è¿‡"] * len(df))
    out_path = path.replace(".csv", "_standardized.csv")
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ç”Ÿæˆ: {out_path}")
    return df

def standardize_working(path, local_map, network_map):
    """working.csvåŒ¹é…å¤„ç†ï¼Œå…ˆæœ¬åœ°åç½‘ç»œ"""
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç† working.csv (åŒ¹é…): {path}")
    df = read_csv_auto(path)
    unmatched = set()
    std_names, statuses = [], []

    for name in df.iloc[:, 0].astype(str):
        std, status = match_name_working(name, local_map, network_map)
        std_names.append(std)
        statuses.append(status)
        if status.startswith("åŒ¹é…åº¦ä½"):
            unmatched.add(name)

    df.insert(0, "æ ‡å‡†é¢‘é“å", std_names)
    df.insert(1, "åŒ¹é…çŠ¶æ€", statuses)

    out_path = path.replace(".csv", "_standardized.csv")
    df.to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ç”Ÿæˆ: {out_path}")
    return df, unmatched

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    update_database()
    network_map = load_name_map()

    my_sum_path = "input/mysource/my_sum.csv"
    working_path = "output/working.csv"

    if not os.path.exists(my_sum_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {my_sum_path}")
        return
    if not os.path.exists(working_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {working_path}")
        return

    # 1. è¯»å–å¹¶â€œæ ‡å‡†åŒ–â€my_sum.csvï¼ˆä¸åŒ¹é…ï¼‰
    df_my_sum = standardize_my_sum(my_sum_path)
    # 2. æ„å»ºæœ¬åœ°åç§°æ˜ å°„ï¼Œç”¨äºåŒ¹é…working.csv
    local_map = build_local_name_map(df_my_sum)
    # 3. æ ‡å‡†åŒ–working.csvï¼Œå…ˆåŒ¹é…æœ¬åœ°å†åŒ¹é…ç½‘ç»œåº“
    df_working, unmatched_working = standardize_working(working_path, local_map, network_map)

    # 4. åˆå¹¶ä¸¤ä¸ªç»“æœç”Ÿæˆæ€»è¡¨
    total_df = pd.concat([df_my_sum, df_working], ignore_index=True)
    total_csv_path = os.path.join(OUTPUT_DIR, "total.csv")
    total_df.to_csv(total_csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ç”Ÿæˆæ€»è¡¨: {total_csv_path}")

    # 5. ç”Ÿæˆé¢‘é“å-åˆ†ç»„æ˜ å°„è¡¨
    if "åˆ†ç»„" in total_df.columns:
        channel_df = total_df[["æ ‡å‡†é¢‘é“å", "åˆ†ç»„"]].drop_duplicates()
        os.makedirs(os.path.dirname(INPUT_CHANNEL_CSV), exist_ok=True)
        channel_df.to_csv(INPUT_CHANNEL_CSV, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²ç”Ÿæˆé¢‘é“åˆ†ç»„æ˜ å°„: {INPUT_CHANNEL_CSV}")
    else:
        print("âš ï¸ æ€»è¡¨ä¸­æœªæ‰¾åˆ°â€œåˆ†ç»„â€åˆ—ï¼Œæ— æ³•ç”Ÿæˆé¢‘é“åˆ†ç»„æ˜ å°„æ–‡ä»¶")

    # 6. unmatchedé¢‘é“è¾“å‡º
    if unmatched_working:
        report_path = os.path.join(OUTPUT_DIR, "unmatched_channels.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            for ch in sorted(unmatched_working):
                f.write(ch + "\n")
        print(f"âš ï¸ working.csv ä¸­åŒ¹é…åº¦ä½çš„é¢‘é“ {len(unmatched_working)} ä¸ªï¼Œå·²ä¿å­˜è‡³ {report_path}")
    else:
        print("ğŸ‰ working.csv ä¸­æ‰€æœ‰é¢‘é“åŒ¹é…åº¦è¾¾æ ‡")

if __name__ == "__main__":
    main()