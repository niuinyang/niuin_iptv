import os
import re
import pandas as pd
import csv
import chardet
from rapidfuzz import process

IPTV_DB_PATH = "./iptv-database"
INPUT_MY = "input/mysource/my_sum.csv"
INPUT_WORKING = "output/working.csv"
OUTPUT_TOTAL = "output/total.csv"
OUTPUT_CHANNEL = "input/channel.csv"

def detect_encoding_and_convert_utf8(filepath):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼Œéutf-8æ—¶è½¬ä¸ºutf-8å¹¶è¦†ç›–"""
    with open(filepath, "rb") as f:
        rawdata = f.read()
    result = chardet.detect(rawdata)
    enc = result['encoding']
    if enc is None:
        enc = 'utf-8'
    if enc.lower() != 'utf-8':
        print(f"ğŸ”„ æ£€æµ‹åˆ° {filepath} ç¼–ç ä¸º {enc}ï¼Œæ­£åœ¨è½¬æ¢ä¸º UTF-8...")
        text = rawdata.decode(enc)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"âœ… å·²è½¬ç å¹¶è¦†ç›–ä¿å­˜ä¸º UTF-8: {filepath}")
    else:
        print(f"âœ… æ–‡ä»¶ {filepath} å·²æ˜¯ UTF-8 ç¼–ç ")

def load_name_map():
    """åŠ è½½iptv-orgæ•°æ®åº“é¢‘é“åå’Œåˆ«åæ˜ å°„"""
    name_map = {}
    path = os.path.join(IPTV_DB_PATH, "data", "channels.csv")
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            std_name = row["name"].strip()
            name_map[std_name.lower()] = std_name
            others = row.get("other_names", "")
            for alias in others.split(","):
                alias = alias.strip()
                if alias:
                    name_map[alias.lower()] = std_name
    print(f"ğŸ“š å·²åŠ è½½ {len(name_map)} ä¸ªåç§°æ˜ å°„")
    return name_map

def clean_channel_name(name: str) -> str:
    """å»é™¤é¢‘é“åä¸­æ‹¬å·()å’Œä¸­æ‹¬å·[]åŠé‡Œé¢å†…å®¹"""
    if not isinstance(name, str):
        return ""
    name = re.sub(r'\([^)]*?\)', '', name)  # å»é™¤()
    name = re.sub(r'\[[^\]]*?\]', '', name)  # å»é™¤[]
    return name.strip()

def get_std_name_with_score(name, name_map, threshold=80):
    name_lower = name.lower()
    if name_lower in name_map:
        return name_map[name_lower], 100
    choices = list(name_map.keys())
    match, score, _ = process.extractOne(name_lower, choices)
    if score >= threshold:
        return name_map[match], score
    else:
        return name, score

def standardize_my_sum(file_path):
    """my_sum.csv ä¸åŒ¹é…ï¼Œæ ‡å‡†åå³ä¸ºåŸåï¼Œä¿ç•™æ£€æµ‹æ—¶é—´"""
    df = pd.read_csv(file_path, encoding="utf-8")
    original_names = df.iloc[:, 0].astype(str).str.strip()
    df.insert(0, 'final_name', original_names)
    # ä¸æ”¹å˜æ£€æµ‹æ—¶é—´åˆ—
    df.to_csv(file_path.replace(".csv", "_standardized.csv"), index=False, encoding="utf-8")
    print(f"âœ… {file_path} æ ‡å‡†åŒ–å®Œæˆï¼Œè¾“å‡ºåˆ° {file_path.replace('.csv', '_standardized.csv')}")
    return df

def standardize_working(file_path, name_map, my_sum_names_set):
    """working.csv å…ˆæ¸…ç†åå­—ï¼Œå†åŒ¹é…ï¼ˆä¼˜å…ˆåŒ¹é…my_sum.csvåå­—ï¼‰ï¼Œä¿ç•™æ£€æµ‹æ—¶é—´"""
    df = pd.read_csv(file_path, encoding="utf-8")
    original_names = df.iloc[:, 0].astype(str).str.strip()
    clean_names = original_names.apply(clean_channel_name)

    std_names = []
    remarks = []
    for orig_name, clean_name in zip(original_names, clean_names):
        if orig_name in my_sum_names_set:
            std_names.append(orig_name)
            remarks.append("è‡ªæœ‰æºä¼˜å…ˆ")
        else:
            std_name, score = get_std_name_with_score(clean_name, name_map)
            if score < 95:
                std_names.append(orig_name)
                remarks.append(f"æ¨¡ç³ŠåŒ¹é…({score:.0f})ä½äº95")
            else:
                std_names.append(std_name)
                remarks.append(f"æ¨¡ç³ŠåŒ¹é…({score:.0f})")
    df.insert(0, "final_name", std_names)
    df["match_remark"] = remarks
    # ä¿ç•™æ£€æµ‹æ—¶é—´åŸåˆ—ï¼Œä¸åšä¿®æ”¹
    df.to_csv(file_path.replace(".csv", "_standardized.csv"), index=False, encoding="utf-8")
    print(f"âœ… {file_path} æ ‡å‡†åŒ–å®Œæˆï¼Œè¾“å‡ºåˆ° {file_path.replace('.csv', '_standardized.csv')}")
    return df

def save_channel_csv(my_sum_df, working_df):
    """æå–æ ‡å‡†åŒ–åå’Œåˆ†ç»„ä¸¤åˆ—åˆå¹¶è¾“å‡ºåˆ° channel.csv"""
    dfs = []
    for df in [my_sum_df, working_df]:
        cols = df.columns.tolist()
        # é¢‘é“ååœ¨final_nameï¼Œåˆ†ç»„åˆ—å¯èƒ½æ˜¯â€œåˆ†ç»„â€æˆ–è€…â€œgroupâ€ï¼Œå°è¯•è¯†åˆ«
        group_col = None
        for c in ["åˆ†ç»„", "group"]:
            if c in df.columns:
                group_col = c
                break
        if group_col is None:
            raise ValueError("æ— æ³•æ‰¾åˆ°åˆ†ç»„åˆ—")
        dfs.append(df[["final_name", group_col]].rename(columns={group_col:"åˆ†ç»„"}))
    combined = pd.concat(dfs, ignore_index=True)
    combined.drop_duplicates(inplace=True)
    combined.to_csv(OUTPUT_CHANNEL, index=False, encoding="utf-8")
    print(f"âœ… å·²è¾“å‡ºé¢‘é“åå’Œåˆ†ç»„åˆ° {OUTPUT_CHANNEL}")

def save_total_csv(my_sum_df, working_df):
    """åˆå¹¶ä¸¤ä¸ªdfï¼Œè¾“å‡ºtotal.csv"""
    combined = pd.concat([my_sum_df, working_df], ignore_index=True)
    combined.to_csv(OUTPUT_TOTAL, index=False, encoding="utf-8")
    print(f"âœ… å·²è¾“å‡ºåˆå¹¶æ€»è¡¨åˆ° {OUTPUT_TOTAL}")

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ ‡å‡†åŒ–åŒ¹é…æµç¨‹...")

    # å…ˆç¡®ä¿è¾“å…¥æ–‡ä»¶ç¼–ç æ­£ç¡®
    for f in [INPUT_MY, INPUT_WORKING]:
        detect_encoding_and_convert_utf8(f)

    name_map = load_name_map()

    # å¤„ç†my_sum.csvï¼Œç›´æ¥æ ‡å‡†åŒ–ä¸ºåŸå
    my_sum_df = standardize_my_sum(INPUT_MY)
    my_sum_names_set = set(my_sum_df.iloc[:, 0].astype(str).str.strip())

    # å¤„ç†working.csvï¼Œå…ˆæ¸…ç†å†åŒ¹é…ï¼Œä¼˜å…ˆåŒ¹é…my_sumå
    working_df = standardize_working(INPUT_WORKING, name_map, my_sum_names_set)

    # ç”Ÿæˆé¢‘é“åå’Œåˆ†ç»„çš„channel.csv
    save_channel_csv(my_sum_df, working_df)

    # åˆå¹¶è¾“å‡ºtotal.csv
    save_total_csv(my_sum_df, working_df)

if __name__ == "__main__":
    main()