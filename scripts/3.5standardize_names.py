import csv
import os
import pandas as pd
from rapidfuzz import process
import re
import chardet

IPTV_DB_PATH = "./iptv-database"

INPUT_MY = "input/mysource/my_sum.csv"
INPUT_WORKING = "output/working.csv"
OUTPUT_TOTAL = "output/total.csv"
INPUT_CHANNEL = "input/channel.csv"   # ä½œä¸ºè¾“å…¥çš„channel.csv
OUTPUT_CHANNEL = "input/channel.csv"  # è¦†ç›–å†™å›channel.csv
MANUAL_MAP_PATH = "input/manual_map.csv"    # äººå·¥æ˜ å°„æ–‡ä»¶è·¯å¾„
UNMATCHED_PATH = "unmatched_channels.csv"  # å¯¼å‡ºæœªåŒ¹é…é¢‘é“åˆ—è¡¨ï¼ˆå¤‡ç”¨ï¼‰

def safe_read_csv(path):
    with open(path, "rb") as f:
        raw = f.read()
    result = chardet.detect(raw)
    enc = result["encoding"]
    if enc is None:
        enc = "utf-8"
    if enc.lower() != "utf-8":
        text = raw.decode(enc, errors="ignore")
        with open(path, "w", encoding="utf-8") as f:
            f.write(text)
    return pd.read_csv(path, encoding="utf-8")

def load_name_map():
    name_map = {}
    path = os.path.join(IPTV_DB_PATH, "data", "channels.csv")
    with open(path, encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            std_name = row["name"].strip()
            name_map[std_name.lower()] = std_name
            others = row.get("other_names", "")
            for alias in others.split(","):
                alias = alias.strip()
                if alias:
                    name_map[alias.lower()] = std_name
    return name_map

def load_manual_map(path=MANUAL_MAP_PATH):
    manual_map = {}
    if not os.path.exists(path):
        # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œåˆ›å»ºå¸¦è¡¨å¤´çš„ç©ºæ–‡ä»¶
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8-sig", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"])
        return manual_map

    # è‡ªåŠ¨æ£€æµ‹ç¼–ç è¯»å–
    with open(path, "rb") as f:
        raw = f.read()
    result = chardet.detect(raw)
    enc = result["encoding"] if result["encoding"] else "utf-8"

    text = raw.decode(enc, errors="ignore")

    from io import StringIO
    f = StringIO(text)
    reader = csv.DictReader(f)
    for row in reader:
        raw_name = row.get("åŸå§‹åç§°", "").strip()
        std_name = row.get("æ ‡å‡†åç§°", "").strip()
        if raw_name and std_name:
            manual_map[raw_name.lower()] = std_name
    return manual_map

def clean_channel_name(name):
    if not isinstance(name, str):
        return ""
    # å»é™¤ï¼ˆï¼‰å’Œã€ã€‘åŠé‡Œé¢å†…å®¹
    name = re.sub(r"[\(\ï¼ˆ][^\)\ï¼‰]*[\)\ï¼‰]", "", name)
    name = re.sub(r"[\[\ã€][^\]\ã€‘]*[\]\ã€‘]", "", name)
    # å»é™¤ 24/7ã€7*24ã€7x24ï¼Œä»¥åŠå¸¦ not çš„æƒ…å†µï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    name = re.sub(r"\b(not\s*)?(24/7|7\*24|7x24)\b", "", name, flags=re.I)
    return name.strip()

def normalize_name_for_match(name):
    if not isinstance(name, str):
        return ""
    name = clean_channel_name(name)
    # å»é™¤è¿å­—ç¬¦å’Œç©ºæ ¼ï¼Œæ–¹ä¾¿åŒ¹é…
    name = re.sub(r"[-\s]", "", name)
    return name.lower()

def get_std_name(name, name_map, threshold=95):
    name_lower = name.lower()
    if name_lower in name_map:
        return name_map[name_lower], 100.0, "ç²¾ç¡®åŒ¹é…"
    choices = list(name_map.keys())
    match, score, _ = process.extractOne(name_lower, choices)
    if score >= threshold:
        return name_map[match], score, "æ¨¡ç³ŠåŒ¹é…"
    else:
        return name, score, "æœªåŒ¹é…"

def standardize_my_sum(my_sum_df):
    my_sum_df['final_name'] = my_sum_df.iloc[:,0].astype(str)
    my_sum_df['match_info'] = "è‡ªæœ‰æº"
    my_sum_df['original_channel_name'] = my_sum_df.iloc[:,0].astype(str)
    return my_sum_df

def standardize_working(working_df, my_sum_df, name_map, manual_map):
    working_df['original_channel_name'] = working_df.iloc[:, 0].astype(str)
    working_df['clean_name'] = working_df['original_channel_name'].apply(clean_channel_name)
    my_name_dict = dict(zip(my_sum_df.iloc[:,0].apply(normalize_name_for_match), my_sum_df['final_name']))

    total = len(working_df)
    final_names = []
    match_infos = []
    matched_count = 0
    unmatched_count = 0

    print(f"ğŸ”„ å¼€å§‹å¯¹ working.csv å…± {total} æ¡è®°å½•è¿›è¡Œæ ‡å‡†åŒ–åŒ¹é…...")

    for idx, (orig_name, clean_name) in enumerate(zip(working_df['original_channel_name'], working_df['clean_name']), 1):
        orig_name_lower = orig_name.lower()
        clean_name_lower = normalize_name_for_match(clean_name)

        # ä¼˜å…ˆæ£€æŸ¥äººå·¥æ˜ å°„
        if orig_name_lower in manual_map:
            std_name = manual_map[orig_name_lower]
            match_info = "äººå·¥åŒ¹é…"
            matched_count += 1
        elif clean_name_lower in my_name_dict:
            std_name = my_name_dict[clean_name_lower]
            match_info = "è‡ªæœ‰æºåŒ¹é…"
            matched_count += 1
        else:
            # è¿™é‡Œç”¨ process.extractOne æ‰¾æœ€æ¥è¿‘çš„åŒ¹é…é¡¹
            choices = list(name_map.keys())
            match, score, _ = process.extractOne(clean_name_lower, choices)
            if score >= 95:
                std_name = name_map[match]
                match_info = "æ¨¡ç³ŠåŒ¹é…"
                matched_count += 1
            elif score > 0:  # ä½åŒ¹é…ä½†ä¸æ˜¯0åˆ†ï¼Œè§†ä¸ºä½åŒ¹é…ï¼Œå†™æ‹ŸåŒ¹é…é¢‘é“å
                std_name = clean_name.title()  # ç”¨åˆæ­¥æ ‡å‡†å
                match_info = f"ä½åŒ¹é…;æ‹ŸåŒ¹é…é¢‘é“:{name_map[match]}"
                unmatched_count += 1
            else:
                std_name = clean_name.title()
                match_info = "æœªåŒ¹é…"
                unmatched_count += 1

        final_names.append(std_name)
        match_infos.append(match_info)

        if idx % 200 == 0 or idx == total:
            print(f"å·²å¤„ç† {idx}/{total} æ¡ï¼Œå·²åŒ¹é… {matched_count} æ¡ï¼ŒæœªåŒ¹é… {unmatched_count} æ¡")

    working_df['final_name'] = final_names
    working_df['match_info'] = match_infos
    return working_df

def export_unmatched_for_manual(working_df, manual_map_path=MANUAL_MAP_PATH):
    # é€‰å‡ºæœªåŒ¹é…å’Œä½åŒ¹é…çš„è®°å½•
    unmatched_df = working_df[working_df['match_info'].str.contains("æœªåŒ¹é…|ä½åŒ¹é…")]

    # å–åŸå§‹é¢‘é“åå’Œæ‹ŸåŒ¹é…é¢‘é“ï¼ˆå¦‚æœæœ‰ï¼‰
    # æ‹ŸåŒ¹é…é¢‘é“ä» match_info å­—æ®µä¸­æå–
    def extract_candidate(info):
        import re
        m = re.search(r"æ‹ŸåŒ¹é…é¢‘é“:(.*)", info)
        return m.group(1).strip() if m else ""

    unmatched_df = unmatched_df[['original_channel_name', 'match_info']].drop_duplicates()
    unmatched_df['æ‹ŸåŒ¹é…é¢‘é“'] = unmatched_df['match_info'].apply(extract_candidate)
    unmatched_df.rename(columns={'original_channel_name':'åŸå§‹åç§°'}, inplace=True)
    unmatched_df['æ ‡å‡†åç§°'] = ""

    # åªä¿ç•™3åˆ—ï¼Œä¸”é¡ºåºå›ºå®š
    unmatched_df = unmatched_df[['åŸå§‹åç§°', 'æ ‡å‡†åç§°', 'æ‹ŸåŒ¹é…é¢‘é“']]

    # è¯»å–å·²å­˜åœ¨çš„ manual_mapï¼Œé¿å…é‡å¤å†™å…¥
    if os.path.exists(manual_map_path):
        existing = pd.read_csv(manual_map_path, encoding="utf-8-sig")
        existing_names = existing['åŸå§‹åç§°'].str.lower().tolist()
    else:
        # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œåˆ›å»ºå¸¦è¡¨å¤´çš„ç©ºæ–‡ä»¶
        os.makedirs(os.path.dirname(manual_map_path), exist_ok=True)
        with open(manual_map_path, "w", encoding="utf-8-sig", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"])
        existing = pd.DataFrame(columns=["åŸå§‹åç§°", "æ ‡å‡†åç§°", "æ‹ŸåŒ¹é…é¢‘é“"])
        existing_names = []

    # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„åŸå§‹åç§°
    new_rows = unmatched_df[~unmatched_df['åŸå§‹åç§°'].str.lower().isin(existing_names)]

    if not new_rows.empty:
        # è¿½åŠ å†™å…¥æ–‡ä»¶ï¼Œåªå†™è¿™3åˆ—
        new_rows.to_csv(manual_map_path, mode='a', index=False, header=not os.path.exists(manual_map_path), encoding="utf-8-sig")
        print(f"ğŸ”” æœ‰ {len(new_rows)} ä¸ªæœªåŒ¹é…æˆ–ä½åŒ¹é…é¢‘é“å†™å…¥åˆ° {manual_map_path}ï¼Œè¯·æ‰‹åŠ¨è¡¥å…¨æ ‡å‡†åç§°ã€‚")
    else:
        print(f"ğŸ”” æ— æ–°å¢æœªåŒ¹é…æˆ–ä½åŒ¹é…é¢‘é“å†™å…¥ {manual_map_path}ã€‚")

def build_total_df(df):
    def safe_col(name_list):
        for name in name_list:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * len(df))

    return pd.DataFrame({
        "é¢‘é“å": df.get("final_name", df.iloc[:, 0]),
        "åœ°å€": safe_col(["åœ°å€"]),
        "æ¥æº": safe_col(["æ¥æº"]),
        "æ£€æµ‹æ—¶é—´": safe_col(["æ£€æµ‹æ—¶é—´", "æ£€æµ‹æ—¶é—´(å»¶è¿Ÿ)"]),
        "å›¾æ ‡": safe_col(["å›¾æ ‡"]),
        "åˆ†ç»„": safe_col(["åˆ†ç»„"]),
        "åŒ¹é…ä¿¡æ¯": safe_col(["match_info"]),
        "åŸå§‹é¢‘é“å": safe_col(["original_channel_name"])
    })

def save_standardized_my_sum(df):
    def safe_col(name_list):
        for name in name_list:
            if name in df.columns:
                return df[name]
        return pd.Series([""] * len(df))

    out_df = pd.DataFrame({
        "é¢‘é“å": df.get("final_name", df.iloc[:, 0]),
        "åœ°å€": safe_col(["åœ°å€"]),
        "æ¥æº": safe_col(["æ¥æº"]),
        "æ£€æµ‹æ—¶é—´": safe_col(["æ£€æµ‹æ—¶é—´", "æ£€æµ‹æ—¶é—´(å»¶è¿Ÿ)"]),
        "å›¾æ ‡": safe_col(["å›¾æ ‡"]),
        "åˆ†ç»„": safe_col(["åˆ†ç»„"]),
        "åŒ¹é…ä¿¡æ¯": safe_col(["match_info"]),
        "åŸå§‹é¢‘é“å": safe_col(["original_channel_name"])
    })
    # ä¿å­˜ä¸º utf-8-sig ç¼–ç çš„ CSV æ–‡ä»¶
    out_df.to_csv("input/mysource/my_sum_standardized.csv", index=False, encoding="utf-8-sig")
    print("âœ… å·²ä¿å­˜æ–‡ä»¶ï¼šinput/mysource/my_sum_standardized.csv")

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ ‡å‡†åŒ–åŒ¹é…æµç¨‹...")

    my_sum_df = safe_read_csv(INPUT_MY)
    working_df = safe_read_csv(INPUT_WORKING)

    print(f"è¯»å–æºæ–‡ä»¶ï¼š\n  ğŸ“ {INPUT_MY}\n  ğŸ“ {INPUT_WORKING}")

    name_map = load_name_map()
    manual_map = load_manual_map()
    print(f"âœ… æ•°æ®åº“åŠ è½½å®Œæˆï¼Œæ˜ å°„æ€»æ•°ï¼š{len(name_map)}ï¼Œäººå·¥æ˜ å°„æ¡æ•°ï¼š{len(manual_map)}")

    my_sum_df = standardize_my_sum(my_sum_df)
    save_standardized_my_sum(my_sum_df)

    working_df = standardize_working(working_df, my_sum_df, name_map, manual_map)

    export_unmatched_for_manual(working_df)

    my_sum_out = build_total_df(my_sum_df)
    working_out = build_total_df(working_df)

    total_df = pd.concat([my_sum_out, working_out], ignore_index=True)

    # ä¿å­˜åˆå¹¶æ–‡ä»¶ï¼ˆutf-8-sigç¼–ç ï¼‰
    total_df.to_csv(OUTPUT_TOTAL, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜æ–‡ä»¶ï¼š{OUTPUT_TOTAL}")

    # è¯»å–å·²æœ‰é¢‘é“åˆ—è¡¨ä½œä¸ºè¾“å…¥
    if os.path.exists(INPUT_CHANNEL):
        existing_channel_df = pd.read_csv(INPUT_CHANNEL, encoding="utf-8-sig")
    else:
        existing_channel_df = pd.DataFrame(columns=["é¢‘é“å", "åˆ†ç»„"])

    # -- æ–°å¢ä»£ç å¼€å§‹ --
    # 1. æ„é€  manual_map æ˜ å°„å­—å…¸ï¼ˆå°å†™é”®ï¼‰
    manual_map_lower = {k.lower(): v for k, v in manual_map.items()}

    # 2. æ›¿æ¢ existing_channel_df ä¸­çš„æ—§åä¸ºæ ‡å‡†åï¼Œä¿ç•™åˆ†ç»„
    def replace_name(row):
        old_name_lower = row["é¢‘é“å"].lower()
        if old_name_lower in manual_map_lower:
            return manual_map_lower[old_name_lower]
        return row["é¢‘é“å"]

    existing_channel_df["é¢‘é“å"] = existing_channel_df.apply(replace_name, axis=1)

    # 3. å»é‡ï¼Œä¿ç•™é¦–æ¬¡å‡ºç°ï¼ˆåŸæœ‰é¢‘é“ä¼˜å…ˆï¼‰
    existing_channel_df.drop_duplicates(subset=["é¢‘é“å"], keep="first", inplace=True)

    # 4. total_dfä¸­çš„é¢‘é“åé›†åˆ
    total_channels = total_df[["é¢‘é“å", "åˆ†ç»„"]]

    existing_names = set(existing_channel_df["é¢‘é“å"])

    # 5. æ‰¾å‡º total_df ä¸­ä¸å­˜åœ¨äº existing_channel_df çš„æ–°é¢‘é“
    new_channels_df = total_channels[~total_channels["é¢‘é“å"].isin(existing_names)].copy()

    # 6. ç»™æ–°é¢‘é“ç»Ÿä¸€æ ‡è®°â€œæœªåˆ†ç±»â€
    new_channels_df["åˆ†ç»„"] = "æœªåˆ†ç±»"

    # 7. åˆå¹¶ï¼Œä¿æŒåŸæœ‰é¢‘é“é¡ºåºï¼Œè¿½åŠ æ–°é¢‘é“
    combined_channel_df = pd.concat([existing_channel_df, new_channels_df], ignore_index=True)

    # 8. æŒ‰é¢‘é“åå»é‡ï¼Œä¿ç•™é¦–æ¬¡å‡ºç°ï¼ˆåŸæœ‰é¢‘é“ä¼˜å…ˆï¼‰
    combined_channel_df.drop_duplicates(subset=["é¢‘é“å"], keep="first", inplace=True)

    # ä¿å­˜æ›´æ–°çš„ channel.csvï¼ˆutf-8-sigç¼–ç ï¼‰
    combined_channel_df.to_csv(OUTPUT_CHANNEL, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜æ–‡ä»¶ï¼š{OUTPUT_CHANNEL}")
    # -- æ–°å¢ä»£ç ç»“æŸ --

if __name__ == "__main__":
    main()
